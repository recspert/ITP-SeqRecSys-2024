{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "username = 'recspert'\n",
    "repo = 'ITP-SeqRecSys-2024'\n",
    "\n",
    "# remove local directory if it already exists\n",
    "if os.path.isdir(repo):\n",
    "    !rm -rf {repo}\n",
    "\n",
    "!git clone https://github.com/{username}/{repo}.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import reindex, leave_one_out\n",
    "\n",
    "from dataprep import transform_indices\n",
    "from evaluation import topn_recommendations\n",
    "\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a recommender systems framework for conducting the necessary experiments, which includes:\n",
    "- training recsys models\n",
    "- generating recommendations\n",
    "- evaluating recommendations quality\n",
    "- performing model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment protocol can be described in terms of 4 functions:\n",
    "\n",
    "```python\n",
    "# building/training a recommender model\n",
    "model_params = build_func(model_config, trainset, trainset_description)\n",
    "\n",
    "# predicting relevance scores for test user-item pairs\n",
    "model_scores = score_func(model_params, testset, testset_description)\n",
    "\n",
    "# generating top-n recommendations using predcted scores\n",
    "model_recoms = recom_func(model_scores, topn)\n",
    "\n",
    "# evaluating quality of recommendations\n",
    "recs_quality = evaluate_func(model_recoms, holdout, holdout_description)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Essentially, this is the main functionality provided by most of the recommender systems frameworks.**  \n",
    "That's why we can say that we're building a simple recsys framework from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_movielens_data(include_time=True)\n",
    "\n",
    "data_description = {\n",
    "    'users':'userid',\n",
    "    'items':'movieid',\n",
    "    'feedback':'rating',\n",
    "    'timestamp':'timestamp'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./plots/split.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the timepoint corresponding to the 95% percentile\n",
    "test_timepoint = data['timestamp'].quantile(\n",
    "    q=0.95, interpolation='nearest'\n",
    ")\n",
    "\n",
    "# interaction after timepoint go to test\n",
    "test_data_ = data.query('timestamp >= @test_timepoint')\n",
    "# interaction before timepoint go to train,\n",
    "# also hiding the interactions of test users\n",
    "# this ensures the warm-start strategy\n",
    "train_data_ = data.query(\n",
    "    'userid not in @test_data_.userid.unique() and timestamp < @test_timepoint'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./plots/truncate.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform user and item ids for convenience, reindex test data\n",
    "training, data_index = transform_indices(train_data_.copy(), 'userid', 'movieid')\n",
    "test_data = reindex(test_data_, data_index['items'], filter_invalid=False)\n",
    "\n",
    "# the items that were not in the training set have itemid -1\n",
    "# let's drop the items with itemid -1 and all consequtive interactions\n",
    "test_data = test_data.sort_values(by=[data_description['users'], data_description['timestamp']])\n",
    "mask = test_data.groupby(data_description['users']).cummin()[data_description['items']] == -1\n",
    "test_data_truncated = test_data[~mask]\n",
    "\n",
    "# also get rid of users who have just 1 interaction\n",
    "interaction_counts = test_data_truncated.groupby(data_description['users']).size()\n",
    "users_to_keep = interaction_counts[interaction_counts >= 2].index\n",
    "test_prepared = test_data_truncated[test_data_truncated[data_description['users']].isin(users_to_keep)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset, holdout = leave_one_out(\n",
    "    test_prepared, target='timestamp', sample_top=True, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating recommendations quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **HitRate** (HR) Ð¸ **Mean Reciprocal Rank** (MRR).  We will also compute **Coverage** (Cov) to evaluate the diversity of recommendations\n",
    "\n",
    "*Note*: In the case of a single holdout item per user the latter coincides with the Average Reciprocal HitRate (ARHR) and Mean Average Precision (MAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{HR} = \\frac{1}{\\text{\\# test users}} \\sum_{\\text{test users}}{hit}, \\quad\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "hit = \n",
    "\\begin{gather*}\n",
    "\\begin{cases}\n",
    "  1 & \\text{if holdout item in top-$n$ recommendations,}\\\\    \n",
    "  0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\end{gather*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{MRR} = \\frac{1}{\\text{\\# test users}} \\sum_{\\text{test users}}{\\frac{1}{\\text{hit rank}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Cov} = \\frac{\\text{\\# unique recommendations}}{\\text{\\# items in catalogue}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topn_recommendations(scores, topn=10):\n",
    "    recommendations = np.apply_along_axis(topidx, 1, scores, topn)\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def topidx(a, topn):\n",
    "    parted = np.argpartition(a, -topn)[-topn:]\n",
    "    return parted[np.argsort(-a[parted])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downvote_seen_items(scores, data, data_description):\n",
    "    assert isinstance(scores, np.ndarray), 'Scores must be a dense numpy array!'\n",
    "    itemid = data_description['items']\n",
    "    userid = data_description['users']\n",
    "    # get indices of observed data, corresponding to scores array\n",
    "    # we need to provide correct mapping of rows in scores array into\n",
    "    # the corresponding user index (which is assumed to be sorted)\n",
    "    row_idx, test_users = pd.factorize(data[userid], sort=True)\n",
    "    assert len(test_users) == scores.shape[0]\n",
    "    col_idx = data[itemid].values\n",
    "    # downvote scores at the corresponding positions\n",
    "    scores[row_idx, col_idx] = scores.min() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(recommended_items, holdout, holdout_description, topn=10):\n",
    "    itemid = holdout_description['items']\n",
    "    holdout_items = holdout[itemid].values\n",
    "    assert recommended_items.shape[0] == len(holdout_items)\n",
    "    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n",
    "    # HR calculation\n",
    "    hr = ...\n",
    "    # MRR calculation\n",
    "    n_test_users = recommended_items.shape[0]\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    mrr = ...\n",
    "    # coverage calculation\n",
    "    n_items = holdout_description['n_items']\n",
    "    cov = ...\n",
    "    return {'hr':hr, 'mrr':mrr, 'cov':cov}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_evaluate_model(Model, model_config, data_dict, data_description):\n",
    "    '''\n",
    "    Builds the model and calculates metric using the holdout set.\n",
    "    Args:\n",
    "        Model (model class): The model class to train\n",
    "        model config (dict): A dictionary with model hyperparameters\n",
    "        data_dict (dict): A dictionary containing data with the following keys:\n",
    "            - 'train' (pd.DataFrame): The input dataframe containing the train user-item interactions.\n",
    "            - 'test' (pd.DataFrame): The input dataframe containing the test user-item interactions.\n",
    "            - 'holdout' (pd.DataFrame): The input dataframe containing the holdout to measure the quality of recommendations.\n",
    "        data_description (dict): A dictionary containing the data description with the following keys:\n",
    "            - 'n_users' (int): The total number of unique users in the data.\n",
    "            - 'n_items' (int): The total number of unique items in the data.\n",
    "            - 'users' (str): The name of the column in the dataframe containing the user ids.\n",
    "            - 'items' (str): The name of the column in the dataframe containing the item ids.\n",
    "            - 'feedback' (str): The name of the column in the dataframe containing the user-item interaction feedback.\n",
    "            - 'timestamp' (str): The name of the column in the dataframe containing the timestamps of interactions.\n",
    "\n",
    "    Returns:\n",
    "        np.array: A numpy array of shape (n_test_users, n_items) containing the scores for each user.\n",
    "    '''\n",
    "    model = Model(model_config)\n",
    "    model.build(data_dict['train'], data_description)\n",
    "    preds = model.recommend(data_dict['test'], data_description)\n",
    "    downvote_seen_items(preds, data_dict['test'], data_description)\n",
    "    recs = topn_recommendations(preds)\n",
    "    metrics = model_evaluate(recs, data_dict['holdout'], data_description)\n",
    "\n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        # for reproducibility, not a hyperparameter\n",
    "        self.seed = model_config['seed']\n",
    "        self.rng = np.random.default_rng(seed=model_config['seed'])\n",
    "    \n",
    "    def build(self, data, data_description):\n",
    "        self.n_items = data.nunique()[data_description['items']]\n",
    "        \n",
    "    def recommend(self, data, data_description):\n",
    "        n_users = data.nunique()[data_description['users']]\n",
    "        n_items = ...\n",
    "        scores = ...\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Popular:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        pass\n",
    "    \n",
    "    def build(self, data, data_description):\n",
    "        item_popularity = data[data_description['items']].value_counts()\n",
    "        n_items = item_popularity.index.max() + 1\n",
    "        popularity_scores = ...\n",
    "        popularity_scores[...] = item_popularity.values\n",
    "        self.popularity_scores = ...\n",
    "        \n",
    "    def recommend(self, data, data_description):\n",
    "        n_users = data.nunique()[data_description['users']]\n",
    "        scores = ...\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'train':training,\n",
    "    'test':testset,\n",
    "    'holdout':holdout\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = {\n",
    "    'users':'userid',\n",
    "    'items':'movieid',\n",
    "    'feedback':'rating',\n",
    "    'timestamp':'timestamp',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_metrics, rand_model = build_evaluate_model(Model=Random, model_config={'seed':2024}, data=data_dict, data_description=data_description)\n",
    "pop_metrics, pop_model = build_evaluate_model(Model=Popular, model_config={}, data=data_dict, data_description=data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interactions_matrix(data, data_description, rebase_users=False):\n",
    "    '''\n",
    "    Converts a pandas dataframe with user-item interactions into a sparse matrix representation.\n",
    "    Allows reindexing user ids, which help ensure data consistency at the scoring stage\n",
    "    (assumes user ids are sorted in the scoring array).\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input dataframe containing the user-item interactions.\n",
    "        data_description (dict): A dictionary containing the data description with the following keys:\n",
    "            - 'n_users' (int): The total number of unique users in the data.\n",
    "            - 'n_items' (int): The total number of unique items in the data.\n",
    "            - 'users' (str): The name of the column in the dataframe containing the user ids.\n",
    "            - 'items' (str): The name of the column in the dataframe containing the item ids.\n",
    "            - 'feedback' (str): The name of the column in the dataframe containing the user-item interaction feedback.\n",
    "        rebase_users (bool, optional): Whether to reindex the user ids to make contiguous index starting from 0. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: A sparse matrix of shape (n_users, n_items) containing the user-item interactions.\n",
    "    '''\n",
    "    n_users = data_description['n_users']\n",
    "    n_items = data_description['n_items']\n",
    "    # get indices of observed data\n",
    "    user_idx = data[data_description['users']].values\n",
    "    if rebase_users: # handle non-contiguous index of test users\n",
    "        # This ensures that all user ids are contiguous and start from 0,\n",
    "        # which helps ensure data consistency at the scoring stage.\n",
    "        user_idx, user_index = pd.factorize(user_idx, sort=True)\n",
    "        n_users = len(user_index)\n",
    "    item_idx = data[data_description['items']].values\n",
    "    feedback = np.ones_like(item_idx)\n",
    "    # construct rating matrix\n",
    "    return csr_matrix((feedback, (user_idx, item_idx)), shape=(n_users, n_items), dtype='f8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{score}_{AR}(u, i) = \\text{PairCount}_{AR}(i_{|I_u|}, i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PairCount}_{AR}(i, j) = |U_i\\cap U_j|\n",
    "$$\n",
    "\n",
    "$I_u$ - interaction history of user $u$, $U_i$ - set of users who interacted with item $i$, $i_{|I_u|}$ - last item of user $u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        pass\n",
    "    \n",
    "    def build(self, data, data_description):\n",
    "        '''\n",
    "        Builds association rules matrix.\n",
    "        '''\n",
    "        self.rules = ...\n",
    "        \n",
    "    def recommend(self, data, data_description):\n",
    "        '''\n",
    "        Generate scores for given data.\n",
    "        '''\n",
    "        # Drop duplicates, keeping the last interaction for each user\n",
    "        \n",
    "        scores = ...\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{score}_{SR}(u, i) = \\text{PairCount}_{SR}(i_{|I_u|} \\rightarrow i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PairCount}_{SR}(j \\rightarrow i) = \\sum_{v\\in U} \\textbf{1}[j\\rightarrow_v i]\n",
    "$$\n",
    "\n",
    "where $j\\rightarrow_u i$ means that item $i$ follows item $j$ in the interaction history of user $u$. $U$ is the set of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SR:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        pass\n",
    "    \n",
    "    def build(self, data, data_description):\n",
    "        'Builds sequential rules of size two'\n",
    "        rules = {}\n",
    "        \n",
    "        # get chronological interaction history for each user\n",
    "        histories = (\n",
    "            data\n",
    "            .sort_values(\n",
    "                by=data_description['timestamp']\n",
    "                )\n",
    "            .groupby(data_description['users'])[data_description['items']]\n",
    "            .apply(list)\n",
    "            )\n",
    "        \n",
    "        self.rules = ...\n",
    "\n",
    "    def recommend(self, data, data_description):\n",
    "        '''\n",
    "        Generate scores for given data.\n",
    "        '''\n",
    "        scores = ...\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_metrics, ar_model = build_evaluate_model(Model=AR, model_config={}, data=data_dict, data_description=data_description)\n",
    "sr_metrics, sr_model = build_evaluate_model(Model=SR, model_config={}, data=data_dict, data_description=data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Sequential Rules':sr_metrics,\n",
    "    'Association Rules':ar_metrics,\n",
    "    'popular':pop_metrics,\n",
    "    'random':rand_metrics\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame.from_dict(results, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successive evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./plots/split.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the timepoint corresponding to the 95% percentile\n",
    "test_timepoint = data['timestamp'].quantile(\n",
    "    q=0.95, interpolation='nearest'\n",
    ")\n",
    "\n",
    "# interaction after timepoint go to test\n",
    "test_data_ = data.query('timestamp >= @test_timepoint')\n",
    "# interaction before timepoint go to train,\n",
    "# also hiding the interactions of test users\n",
    "# this ensures the warm-start strategy\n",
    "train_data_ = data.query(\n",
    "    'userid not in @test_data_.userid.unique() and timestamp < @test_timepoint'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_timepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform user and item ids for convenience, reindex test data\n",
    "training, data_index = transform_indices(train_data_.copy(), 'userid', 'movieid')\n",
    "\n",
    "# reindex items in test set, if item was not in train, assign -1 as itemid\n",
    "test_data = reindex(test_data_, data_index['items'], filter_invalid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successive evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./plots/eval.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the items that were not in the training set have itemid -1\n",
    "# let's drop the items with itemid -1 and all consequtive interactions\n",
    "test_data = test_data.sort_values(by=[data_description['users'], data_description['timestamp']])\n",
    "mask = test_data.groupby(data_description['users']).cummin()[data_description['items']] == -1\n",
    "test_data_truncated = test_data[~mask]\n",
    "\n",
    "# also get rid of users who have just 1 interaction\n",
    "interaction_counts = test_data_truncated.groupby(data_description['users']).size()\n",
    "users_to_keep = interaction_counts[interaction_counts >= 2].index\n",
    "test_prepared = test_data_truncated[test_data_truncated[data_description['users']].isin(users_to_keep)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into validation and test by users\n",
    "\n",
    "unique_users = test_prepared[data_description['users']].unique()\n",
    "np.random.shuffle(unique_users)\n",
    "\n",
    "# split the users into two halves\n",
    "split_index = len(unique_users) // 2\n",
    "users_val = unique_users[:split_index]\n",
    "users_test = unique_users[split_index:]\n",
    "\n",
    "# create val and test\n",
    "test = test_prepared[test_prepared[data_description['users']].isin(users_test)]\n",
    "val = test_prepared[test_prepared[data_description['users']].isin(users_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform successive evaluation, we need to have access to the user's history in chronological order. Let's create a dictionary with users as keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {}\n",
    "for user, item, rating, timestamp in test.values:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downvote_seen_items_sequence(scores, seen_sequence):\n",
    "    assert isinstance(scores, np.ndarray), 'Scores must be a dense numpy array!'\n",
    "    assert scores.shape[0] == len(seen_sequence), 'Scores size is different from sequence length!'\n",
    "    \n",
    "    for i in range(len(seen_sequence)):\n",
    "        scores[i, seen_sequence[:i + 1]] = scores.min() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successive_evaluation(test_dict, model, topn=10):\n",
    "    cum_hits = 0\n",
    "    cum_reciprocal_ranks = 0.\n",
    "    cum_discounts = 0.\n",
    "    unique_recommendations = set()\n",
    "    total_count = 0\n",
    "    \n",
    "    for user in tqdm(test_dict):\n",
    "        seen_seq = ...\n",
    "        test_seq = ...\n",
    "        num_predictions = len(test_seq)\n",
    "        if not num_predictions: # if no test items left - skip user\n",
    "            continue\n",
    "        scores = model.recommend_sequential(test_seq, seen_seq, user)\n",
    "        downvote_seen_items_sequence(scores, test_dict[user][:-1])\n",
    "        predicted_items = topn_recommendations(scores, topn=topn)\n",
    "        \n",
    "        hit_steps, hit_index = np.where(predicted_items == np.atleast_2d(test_seq).T)\n",
    "        unique_recommendations.update(predicted_items.ravel())\n",
    "\n",
    "        num_hits = hit_index.size\n",
    "        if num_hits:\n",
    "            cum_hits += num_hits\n",
    "            cum_reciprocal_ranks += np.sum(1. / (hit_index+1))\n",
    "            cum_discounts += np.sum(1. / np.log2(hit_index+2))\n",
    "\n",
    "        total_count += num_predictions\n",
    "\n",
    "    hr = cum_hits / total_count\n",
    "    mrr = cum_reciprocal_ranks / total_count\n",
    "    dcg = cum_discounts / total_count\n",
    "    cov = len(unique_recommendations) / scores.shape[1]\n",
    "    results = pd.DataFrame(\n",
    "        data = {f'{model.__class__.__name__}': [hr, mrr, dcg, cov]},\n",
    "        index = [f'{metric}@{topn}' for metric in ['HR', 'MRR', 'NDCG', 'COV']]\n",
    "    )\n",
    "    return results\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance of our Association and Sequential Rule methods we should also build some baselines - random and popular-based. This also serves as sanity check for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        # for reproducibility, not a hyperparameter\n",
    "        self.seed = model_config['seed']\n",
    "        self.rng = np.random.default_rng(seed=model_config['seed'])\n",
    "    \n",
    "    def build(self, data, data_description):\n",
    "        self.n_items = data_description['n_items']\n",
    "        \n",
    "    def recommend(self, data, data_description):\n",
    "        n_users = data.nunique()[data_description['users']]\n",
    "        n_items = data_description['n_items']\n",
    "        return self.rng.random((n_users, n_items))\n",
    "    \n",
    "    def recommend_sequential(self, target_seq, seen_seq, user):\n",
    "        return self.rng.random((len(target_seq), self.n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Popular:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        pass\n",
    "    \n",
    "    def build(self, data, data_description):\n",
    "        item_popularity = data[data_description['items']].value_counts()\n",
    "        n_items = item_popularity.index.max() + 1\n",
    "        popularity_scores = np.zeros(n_items,)\n",
    "        popularity_scores[item_popularity.index] = item_popularity.values\n",
    "        self.popularity_scores = popularity_scores\n",
    "        \n",
    "    def recommend(self, data, data_description):\n",
    "        n_users = data.nunique()[data_description['users']]\n",
    "        return np.tile(self.popularity_scores, (n_users, 1))\n",
    "    \n",
    "    def recommend_sequential(self, target_seq, seen_seq, user):\n",
    "        return np.tile(self.popularity_scores, (len(target_seq), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Association Rules (AR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{score}_{AR}(u, i) = \\text{PairCount}_{AR}(i_{|I_u|}, i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PairCount}_{AR}(i, j) = |U_i\\cap U_j|\n",
    "$$\n",
    "\n",
    "$I_u$ - interaction history of user $u$, $U_i$ - set of users who interacted with item $i$, $i_{|I_u|}$ - last item of user $u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        pass\n",
    "    \n",
    "    def build(self, data, data_description):\n",
    "        '''\n",
    "        Builds association rules matrix.\n",
    "        '''\n",
    "        interactions = generate_interactions_matrix(data, data_description)\n",
    "        \n",
    "        similarity = interactions.T.dot(interactions)\n",
    "        similarity.setdiag(0)\n",
    "        similarity.eliminate_zeros()\n",
    "        self.rules = similarity\n",
    "        \n",
    "    def recommend(self, data, data_description):\n",
    "        '''\n",
    "        Generate scores for given data.\n",
    "        '''\n",
    "        # Drop duplicates, keeping the last interaction for each user\n",
    "        data_sorted = data.sort_values(by=[data_description['users'], data_description['timestamp']])\n",
    "        data_last_interaction = data_sorted.drop_duplicates(subset=data_description['users'], keep='last')\n",
    "\n",
    "        interactions = generate_interactions_matrix(data_last_interaction, data_description, rebase_users=True)\n",
    "        return interactions.dot(self.rules).A\n",
    "\n",
    "    def recommend_sequential(self, target_seq, seen_seq, user):\n",
    "        '''\n",
    "        Generate scores for sequential evaluation - \n",
    "        subsequently add 1 item from target sequence to the seen sequence\n",
    "        and generate predictions for the resulting history.\n",
    "        '''\n",
    "        user_profile = ...seen_seq\n",
    "        test_sequence = ...target_seq\n",
    "        scores = ...\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Rules (MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{score}_{SR}(u, i) = \\text{PairCount}_{SR}(i_{|I_u|} \\rightarrow i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PairCount}_{SR}(j \\rightarrow i) = \\sum_{v\\in U} \\textbf{1}[j\\rightarrow_v i]\n",
    "$$\n",
    "\n",
    "where $j\\rightarrow_u i$ means that item $i$ follows item $j$ in the interaction history of user $u$. $U$ is the set of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SR:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        pass\n",
    "    \n",
    "    def build(self, data, data_description):\n",
    "        'Builds sequential rules of size two'\n",
    "        rules = {}\n",
    "        \n",
    "        # get chronological interaction history for each user\n",
    "        histories = (\n",
    "            data\n",
    "            .sort_values(\n",
    "                by=data_description['timestamp']\n",
    "                )\n",
    "            .groupby(data_description['users'])[data_description['items']]\n",
    "            .apply(list)\n",
    "            )\n",
    "        \n",
    "        # count the number of pairs when item j is interacted with right after item i\n",
    "        for history in histories:\n",
    "            for i in range(len(history) - 1):\n",
    "                if (history[i], history[i + 1]) not in rules:\n",
    "                    rules[(history[i], history[i + 1])] = 0\n",
    "                rules[(history[i], history[i + 1])] += 1\n",
    "                \n",
    "        # create a sparse matrix of sequential rules for easier recommendation\n",
    "        items, values = zip(*rules.items())\n",
    "        i1, i2 = zip(*items)\n",
    "        matrix_shape = (data_description['n_items'], data_description['n_items'])\n",
    "        similarity = coo_matrix((values, (list(i1), list(i2))), shape=matrix_shape).tocsr().T\n",
    "        self.rules = similarity\n",
    "\n",
    "    def recommend(self, data, data_description):\n",
    "        '''\n",
    "        Generate scores for given data.\n",
    "        '''\n",
    "        data_sorted = data.sort_values(by=[data_description['users'], data_description['timestamp']])\n",
    "        data_last_interaction = data_sorted.drop_duplicates(subset=data_description['users'], keep='last')\n",
    "        \n",
    "        interactions = generate_interactions_matrix(data_last_interaction, data_description, rebase_users=True)\n",
    "        return interactions.dot(self.rules).A\n",
    "    \n",
    "    def recommend_sequential(self, target_seq, seen_seq, user):\n",
    "        '''\n",
    "        Generate scores for sequential evaluation - \n",
    "        subsequently add 1 item from target sequence to the seen sequence\n",
    "        and generate predictions for the resulting history.\n",
    "        '''\n",
    "        user_profile = ...seen_seq\n",
    "        test_sequence = ...target_seq\n",
    "        scores = ...\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description_rules = {\n",
    "    'n_users':training.nunique()['userid'],\n",
    "    'n_items':training.nunique()['movieid'],\n",
    "    'users':'userid',\n",
    "    'items':'movieid',\n",
    "    'feedback':'rating',\n",
    "    'timestamp':'timestamp'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_model = AR()\n",
    "ar_model.build(training, data_description_rules)\n",
    "results_ar = successive_evaluation(test_dict, ar_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_model = SR()\n",
    "sr_model.build(training, data_description_rules)\n",
    "results_sr = successive_evaluation(test_dict, sr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_model = Popular()\n",
    "pop_model.build(training, data_description_rules)\n",
    "results_pop = successive_evaluation(test_dict, pop_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_model = Random({'seed':0})\n",
    "rand_model.build(training, data_description_rules)\n",
    "results_rand = successive_evaluation(test_dict, rand_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([results_sr, results_ar, results_pop, results_rand], axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be done to improve model's quality?\n",
    "- Allow counting not consequent pairs of items (e.g. count $\\textbf{A} \\rightarrow \\textbf{B}$ and $\\textbf{A}\\rightarrow \\text{C} \\rightarrow \\text{D} \\rightarrow \\textbf{B}$, probably with weighting proportional to the inverse number of items between the pair)\n",
    "- This also may help in the case of sparse datasets, where there is hard to mine association rules (density of ML-1m is about $4\\%$, which is pretty high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, apart from better accuracy metrics (HR and MRR), the sequential rules model provides much more diverse recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the sparsity of rules matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'AR rules density: {ar_model.rules.size / ar_model.rules.shape[0] ** 2:.2f}')\n",
    "print(f'SR rules density: {sr_model.rules.size / sr_model.rules.shape[0] ** 2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the sparsity pattern of the left 100 $\\times$ 100 corner of rules matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, constrained_layout=True)\n",
    "ax[0].spy(ar_model.rules[:100, :100], markersize=1, label='AR')\n",
    "ax[0].set_title('AR')\n",
    "ax[1].spy(sr_model.rules[:100, :100], markersize=1, label='SR')\n",
    "ax[1].set_title('SR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, constrained_layout=True, figsize=(15, 7))\n",
    "\n",
    "im0 = ax[0].imshow(ar_model.rules[:100, :100].toarray(), cmap='Greys')\n",
    "ax[0].set_title('AR Heatmap')\n",
    "divider0 = make_axes_locatable(ax[0])\n",
    "cax0 = divider0.append_axes(\"right\", size=\"5%\", pad=0.0)\n",
    "fig.colorbar(im0, ax=ax[0], cax=cax0)\n",
    "\n",
    "im1 = ax[1].imshow(sr_model.rules[:100, :100].toarray(), cmap='Greys')\n",
    "ax[1].set_title('SR Heatmap')\n",
    "divider1 = make_axes_locatable(ax[1])\n",
    "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.0)\n",
    "fig.colorbar(im1, ax=ax[1], cax=cax1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(sorted(norm(ar_model.rules, axis=1)), label='AR')\n",
    "plt.semilogy(sorted(norm(sr_model.rules, axis=1)), label='SR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
