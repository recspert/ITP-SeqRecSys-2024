{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "username = 'recspert'\n",
    "repo = 'ITP-SeqRecSys-2024'\n",
    "\n",
    "# remove local directory if it already exists\n",
    "if os.path.isdir(repo):\n",
    "    !rm -rf {repo}\n",
    "\n",
    "!git clone https://github.com/{username}/{repo}.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix, coo_matrix, diags\n",
    "\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex\n",
    "\n",
    "%cd {repo}\n",
    "from source.dataprep.dataprep import transform_indices, generate_interactions_matrix\n",
    "from source.evaluation.evaluation import topn_recommendations, model_evaluate, downvote_seen_items, build_evaluate_model\n",
    "from source.helpers.knn import truncate_similarity\n",
    "from source.helpers.similarity import jaccard_similarity, weighted_jaccard_similarity\n",
    "from source.helpers.plot import plot_histories\n",
    "from source.models.basic import Random, Popular\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_movielens_data(include_time=True)\n",
    "data_description = {\n",
    "    'users':'userid',\n",
    "    'items':'movieid',\n",
    "    'feedback':'rating',\n",
    "    'timestamp':'timestamp'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the timepoint corresponding to the 95% percentile\n",
    "test_timepoint = data['timestamp'].quantile(\n",
    "    q=0.95, interpolation='nearest'\n",
    ")\n",
    "\n",
    "# users with interaction after timepoint go to test\n",
    "_test_data_ = data.query('timestamp >= @test_timepoint')\n",
    "test_users = _test_data_[data_description['users']].unique()\n",
    "test_data_ = data.query(\n",
    "    'userid in @test_users'\n",
    ")\n",
    "# interaction before timepoint go to train,\n",
    "# also hiding the interactions of test users\n",
    "# this ensures the warm-start strategy\n",
    "train_data_ = data.query(\n",
    "    'userid not in @test_data_.userid.unique() and timestamp < @test_timepoint'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform user and item ids for convenience, reindex test data\n",
    "training, data_index = transform_indices(train_data_.copy(), data_description['users'], data_description['items'])\n",
    "\n",
    "# reindex items in test set, if item was not in train, assign -1 as itemid\n",
    "test_data = reindex(test_data_, data_index['items'], filter_invalid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[test_data[data_description['items']] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the items that were not in the training set have itemid -1\n",
    "# let's drop the items with itemid -1 and all consequtive interactions\n",
    "test_data = test_data.sort_values(by=[data_description['users'], data_description['timestamp']])\n",
    "mask = test_data.groupby(data_description['users']).cummin()[data_description['items']] == -1\n",
    "test_data_truncated = test_data[~mask]\n",
    "\n",
    "filtered = test_data_truncated[test_data_truncated[data_description['timestamp']] >= test_timepoint]\n",
    "interaction_counts = filtered.groupby(data_description['users']).size()\n",
    "test_users = interaction_counts[interaction_counts >= 2].index.tolist()\n",
    "\n",
    "test_prepared = test_data_truncated[test_data_truncated[data_description['users']].isin(test_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last interaction for test holdout\n",
    "# second-to-last for validation holdout\n",
    "testset_, holdout_ = leave_one_out(\n",
    "    test_prepared, target='timestamp', sample_top=True, random_state=0\n",
    ")\n",
    "testset_valid_, holdout_valid_ = leave_one_out(\n",
    "    testset_, target='timestamp', sample_top=True, random_state=0\n",
    ")\n",
    "\n",
    "# assert the users in testset are the same as in holdout\n",
    "test_users = np.intersect1d(testset_valid_.userid.unique(), holdout_valid_.userid.unique())\n",
    "testset_valid = testset_valid_.query('userid in @test_users').sort_values('userid')\n",
    "holdout_valid = holdout_valid_.query('userid in @test_users').sort_values('userid')\n",
    "\n",
    "# assert the users in testset_valid are the same as in holdout_valid\n",
    "test_users_final = np.intersect1d(testset_valid_.userid.unique(), holdout_valid_.userid.unique())\n",
    "testset = testset_.query('userid in @test_users_final').sort_values('userid')\n",
    "holdout = holdout_.query('userid in @test_users_final').sort_values('userid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = {\n",
    "    'n_users':training.nunique()['userid'],\n",
    "    'n_items':training.nunique()['movieid'],\n",
    "    'users':'userid',\n",
    "    'items':'movieid',\n",
    "    'feedback':'rating',\n",
    "    'timestamp':'timestamp'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules\n",
    "\n",
    "$$\n",
    "\\text{score}_{AR}(u, i) = \\text{PairCount}_{AR}(i_{|I_u|}, i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PairCount}_{AR}(i, j) = |U_i\\cap U_j|\n",
    "$$\n",
    "\n",
    "$I_u$ - interaction history of user $u$, $U_i$ - set of users who interacted with item $i$, $i_{|I_u|}$ - last item of user $u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        pass\n",
    "\n",
    "    def build(self, data, data_description):\n",
    "        '''\n",
    "        Builds association rules matrix.\n",
    "        '''\n",
    "        interactions = generate_interactions_matrix(data, data_description)\n",
    "\n",
    "        similarity = ...\n",
    "        self.rules = similarity\n",
    "\n",
    "    def recommend(self, data, data_description):\n",
    "        '''\n",
    "        Generate scores for given data.\n",
    "        '''\n",
    "        # Drop duplicates, keeping the last interaction for each user\n",
    "        \n",
    "        data_sorted = data.sort_values(by=[data_description['users'], data_description['timestamp']])\n",
    "        data_last_interaction = ...\n",
    "\n",
    "        interactions = generate_interactions_matrix(data_last_interaction, data_description, rebase_users=True)\n",
    "        \n",
    "        scores = ...\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_model = AR()\n",
    "ar_model.build(training, data_description)\n",
    "ar_scores = ar_model.recommend(testset_valid, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential rules\n",
    "\n",
    "$$\n",
    "\\text{score}_{SR}(u, i) = \\sum_{j \\in I_u  \\backslash \\{i\\}} \\textbf{1}[j=i_{|I_u|}]\\text{PairCount}_{SR}(j \\rightarrow i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PairCount}_{SR}(j \\rightarrow i) = \\sum_{v\\in U} \\textbf{1}[j\\rightarrow_v i]\n",
    "$$\n",
    "\n",
    "where $j\\rightarrow_u i$ means that item $i$ follows item $j$ in the interaction history of user $u$. $U$ is the set of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SR:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        pass\n",
    "\n",
    "    def build(self, data, data_description):\n",
    "        'Builds sequential rules of size two'\n",
    "        rules = {}\n",
    "\n",
    "        # get chronological interaction history for each user\n",
    "        histories = (\n",
    "            data\n",
    "            .sort_values(\n",
    "                by=data_description['timestamp']\n",
    "                )\n",
    "            .groupby(data_description['users'])[data_description['items']]\n",
    "            .apply(list)\n",
    "            )\n",
    "\n",
    "        # count the number of pairs when item j is interacted with right after item i\n",
    "        rules = ...\n",
    "\n",
    "        # create a sparse matrix of sequential rules for easier recommendation\n",
    "        items, values = zip(*rules.items())\n",
    "        i1, i2 = zip(*items)\n",
    "        matrix_shape = (data_description['n_items'], data_description['n_items'])\n",
    "        similarity = coo_matrix((values, (list(i1), list(i2))), shape=matrix_shape).tocsr()\n",
    "\n",
    "        self.rules = similarity\n",
    "\n",
    "    def recommend(self, data, data_description):\n",
    "        '''\n",
    "        Generate scores for given data.\n",
    "        '''\n",
    "        data_sorted = data.sort_values(by=[data_description['users'], data_description['timestamp']])\n",
    "        data_last_interaction = data_sorted.drop_duplicates(subset=data_description['users'], keep='last')\n",
    "\n",
    "        interactions = generate_interactions_matrix(data_last_interaction, data_description, rebase_users=True)\n",
    "        \n",
    "        scores = ...\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_model = SR()\n",
    "sr_model.build(training, data_description)\n",
    "sr_scores = sr_model.recommend(testset_valid, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'train':training,\n",
    "    'test':testset,\n",
    "    'holdout':holdout\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_metrics, ar_model = build_evaluate_model(AR, {}, data_dict, data_description)\n",
    "sr_metrics, sr_model = build_evaluate_model(SR, {}, data_dict, data_description)\n",
    "pop_metrics, pop_model = build_evaluate_model(Popular, {}, data_dict, data_description)\n",
    "rand_metrics, rand_model = build_evaluate_model(Random, {}, data_dict, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models = {\n",
    "    'Rand': rand_metrics,\n",
    "    'Pop': pop_metrics,\n",
    "    'AR': ar_metrics,\n",
    "    'SR': sr_metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models_results = pd.DataFrame.from_dict(simple_models).T\n",
    "simple_models_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder on Jaccard Index:\n",
    "\n",
    "$$\n",
    "\\text{jaccard}(u, v) = \\frac{\\sum_i(\\min(u_i, v_i))}{\\sum_i(\\max(u_i, v_i))}\n",
    "$$\n",
    "\n",
    "where $u=(u_1, ..., u_n)^\\top$ and $v=(v_1, ..., v_n)^\\top$ are vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the binary case\n",
    "\n",
    "$$\n",
    "\\text{Sim}(i, j) = \\frac{|U_i\\cap U_j|}{|U_i\\cup U_j|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "|U_i\\cup U_j| = |U_i| + |U_j| - |U_i\\cap U_j|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(A, B):\n",
    "    '''\n",
    "    Computes the jaccard similarity index between the rows of two input matrices.\n",
    "    The matrices are binarized.\n",
    "    Jaccard(u, v) = \\frac{\\sum_{i=1}^k \\min(u_k, v_k)}{\\sum_{i=1}^k \\max(u_k, v_k)}\n",
    "    \n",
    "    Args:\n",
    "        A (scipy.sparse.csr_matrix): n_users_A x n_items\n",
    "        B (scipy.sparse.csr_matrix): n_users_B x n_items\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: A sparse matrix of shape (n_users_A, n_users_B) containing the similarities between users\n",
    "    '''\n",
    "    assert A.shape[1] == B.shape[1]\n",
    "    A_bin = A.astype('bool').astype('int')\n",
    "    B_bin = B.astype('bool').astype('int')\n",
    "\n",
    "    numerator = ...\n",
    "    denominator = ...\n",
    "    similarity = ...\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(type, m1, m2):\n",
    "    if type == 'jaccard':\n",
    "        similarity = jaccard_similarity(m1, m2)\n",
    "    elif type == 'weighted_jaccard':\n",
    "        similarity = weighted_jaccard_similarity(m1, m2)\n",
    "    elif type == 'cosine':\n",
    "        similarity = cosine_similarity(m1, m2, dense_output=False)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown similarity type: {type}')\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode the information about the sequential nature of the interactions, let's build the user-item interaction matrix where non-zero entries mean that user and item interacted, and the value encodes the inverse position of the item in user's interaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequential_matrix(data, data_description, rebase_users=False):\n",
    "    '''\n",
    "    Converts a pandas dataframe with user-item interactions into a sparse matrix representation.\n",
    "    Allows reindexing user ids, which help ensure data consistency at the scoring stage\n",
    "    (assumes user ids are sorted in the scoring array).\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input dataframe containing the user-item interactions.\n",
    "        data_description (dict): A dictionary containing the data description with the following keys:\n",
    "            - 'n_users' (int): The total number of unique users in the data.\n",
    "            - 'n_items' (int): The total number of unique items in the data.\n",
    "            - 'users' (str): The name of the column in the dataframe containing the user ids.\n",
    "            - 'items' (str): The name of the column in the dataframe containing the item ids.\n",
    "            - 'feedback' (str): The name of the column in the dataframe containing the user-item interaction feedback.\n",
    "            - 'timestamp' (str): The name of the column in the dataframe containing the user-item interaction timestamp.\n",
    "        rebase_users (bool, optional): Whether to reindex the user ids to make contiguous index starting from 0. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: A sparse matrix of shape (n_users, n_items) containing the user-item interactions with reciprocal weighting.\n",
    "    '''\n",
    "\n",
    "    data_sorted = data.sort_values(by=[data_description['timestamp']], ascending=False)\n",
    "    data_sorted['reciprocal_rank'] = ...\n",
    "    \n",
    "    n_users = data_description['n_users']\n",
    "    n_items = data_description['n_items']\n",
    "    # get indices of observed data\n",
    "    user_idx = data_sorted[data_description['users']].values\n",
    "    if rebase_users: # handle non-contiguous index of test users\n",
    "        # This ensures that all user ids are contiguous and start from 0,\n",
    "        # which helps ensure data consistency at the scoring stage.\n",
    "        user_idx, user_index = pd.factorize(user_idx, sort=True)\n",
    "        n_users = len(user_index)\n",
    "    item_idx = data_sorted[data_description['items']].values\n",
    "    ranks = data_sorted['reciprocal_rank'].values\n",
    "    # construct the matrix\n",
    "    return csr_matrix((ranks, (user_idx, item_idx)), shape=(n_users, n_items), dtype='f8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adapt the regular KNN for the sequential task, one can encode the positional information of the interactions in user history, and calculate the similarity matrix using the encoded interactions matrix. One of the most simplest ways is to encode reciprocal rank. Also linear weighting can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of nearest neighbor models that can be applied to the sequential recommendation task. All of them were applied to session-based recommendation task, that is why they all named SKNN (Session-based KNN), and we follow the same notation;\n",
    "\n",
    "- Session-based KNN (SKNN): we compute similarities using binary interaction matrix, and generate predictions using weighted interactions matrix. \n",
    "- Vector multiplication Session-based KNN (V-SKNN): the idea is to put more emphasis on the more recent events of interaction history when computing similarities. \n",
    "- Sequential Session-based KNN (S-SKNN): the similarity is computed as in V-SKNN, but during the scoring step we give more weight to the interactions later in history. More formally, user-based scoring:\n",
    "$$\n",
    "\\text{score}_{\\text{S-SKNN}}(u, i) = \\sum_{v\\in N_i(u)} \\text{sim}(u, v) w(i) a_{vi}\n",
    "$$\n",
    "where $N_i(u)$ is the neighborhood of user $u$; $w(i)$ is the weighting function, which takes into account the order of interactions; $a_{ui}$ is interaction between user $u$ and item $i$.\n",
    "\n",
    "We follow the same notation as in [Ludewig](https://arxiv.org/pdf/1803.09587). This paper investigate next-session prediction task. Comparison in session-based setup can be found there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKNN:\n",
    "$$\n",
    "R = \\text{Sim}(A_{\\text{bin}}, A_{\\text{bin}}) A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKNN:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        self.similarity_type = model_config['similarity']\n",
    "        self.n_neighbors = model_config['n_neighbors']\n",
    "\n",
    "    def build(self, data, data_description):\n",
    "        ...\n",
    "\n",
    "    def recommend(self, test_data, data_description):\n",
    "        test_interactions = ...\n",
    "        full_similarity = compute_similarity(self.similarity_type, ...)\n",
    "        similarity = truncate_similarity(similarity=full_similarity, k=self.n_neighbors)\n",
    "        scores = ...\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sknn_model = SKNN({'similarity':'jaccard', 'n_neighbors':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sknn_model.build(training, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sknn_model.recommend(testset_valid, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.nunique(), testset_valid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V-SKNN:\n",
    "$$\n",
    "R = \\text{Sim}(A, A) A_{\\text{bin}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class V_SKNN:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        self.similarity_type = model_config['similarity']\n",
    "        self.n_neighbors = model_config['n_neighbors']\n",
    "\n",
    "    def build(self, data, data_description):\n",
    "        interactions = generate_sequential_matrix(data, data_description)\n",
    "        self.interactions = interactions\n",
    "\n",
    "    def recommend(self, test_data, data_description):\n",
    "        test_interactions = ...\n",
    "        full_similarity = compute_similarity(self.similarity_type, test_interactions, self.interactions)\n",
    "        similarity = truncate_similarity(similarity=full_similarity, k=self.n_neighbors)\n",
    "        scores = ...\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-SKNN:\n",
    "$$\n",
    "R = \\text{Sim}(A, A) A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S_SKNN:\n",
    "    def __init__(self, model_config=None) -> None:\n",
    "        self.similarity_type = model_config['similarity']\n",
    "        self.n_neighbors = model_config['n_neighbors']\n",
    "        \n",
    "    def build(self, data, data_description):\n",
    "        interactions = generate_sequential_matrix(data, data_description)\n",
    "        self.interactions = interactions\n",
    "        \n",
    "    def recommend(self, test_data, data_description):\n",
    "        test_interactions = generate_sequential_matrix(...)\n",
    "        full_similarity = compute_similarity(self.similarity_type, test_interactions, self.interactions)\n",
    "        similarity = truncate_similarity(similarity=full_similarity, k=self.n_neighbors)\n",
    "        scores = ...\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function enables random search over the model hyperparameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polara.evaluation.pipelines import random_grid\n",
    "import random\n",
    "\n",
    "def find_best_model(Model_class, model_configs, data, data_description, target_metric='hr', gridsearch_size=None, seed=2024):\n",
    "    history = {}\n",
    "    \n",
    "    if gridsearch_size:\n",
    "        # seed is for reproducibility of config sampling\n",
    "        random.seed(seed)\n",
    "        param_grid, param_names = random_grid(model_configs, n=gridsearch_size)\n",
    "    else:\n",
    "        # perform the full search\n",
    "        whole_grid_size = 1\n",
    "        for lst in model_configs.values():\n",
    "            whole_grid_size *= len(lst)\n",
    "        param_grid, param_names = random_grid(model_configs, n=whole_grid_size)\n",
    "        \n",
    "\n",
    "    for config in tqdm(param_grid):\n",
    "        # for each sampled config measure the quality of the model on val set\n",
    "        # and save the results in a dictionary\n",
    "        current_config = dict(zip(param_names, config))\n",
    "        data_dict_val = {\n",
    "            'train':data['train'],\n",
    "            'test':data['val'],\n",
    "            'holdout':data['holdout_val']\n",
    "        }\n",
    "        \n",
    "        metrics, _ = build_evaluate_model(...)\n",
    "        history[config] = metrics\n",
    "\n",
    "    best_config = dict(zip(\n",
    "        param_names,\n",
    "        max(history, key=lambda x: history[x][target_metric]),\n",
    "    ))\n",
    "    data_dict_test = {\n",
    "        'train':data['train'], \n",
    "        'test':data['test'],\n",
    "        'holdout':data['holdout']\n",
    "    }\n",
    "    test_metrics, best_model = build_evaluate_model(...)\n",
    "\n",
    "    return test_metrics, best_model, history, param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'train':training,\n",
    "    'val':testset_valid,\n",
    "    'holdout_val':holdout_valid,\n",
    "    'test':testset,\n",
    "    'holdout':holdout,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_configs = {\n",
    "    'similarity':['weighted_jaccard', 'jaccard', 'cosine'],\n",
    "    'n_neighbors':np.arange(50, 401, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sknn, best_sknn_model, history_sknn, param_names_sknn = find_best_model(SKNN,\n",
    "                                                    knn_configs,\n",
    "                                                    data_dict,\n",
    "                                                    data_description,\n",
    "                                                    target_metric='hr')\n",
    "\n",
    "print(max(history_sknn, key=lambda x: history_sknn[x]['hr']), metrics_sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_v_sknn, best_v_sknn_model, history_v_sknn, param_names_v_sknn = find_best_model(V_SKNN,\n",
    "                                                    knn_configs,\n",
    "                                                    data_dict,\n",
    "                                                    data_description,\n",
    "                                                    target_metric='hr')\n",
    "\n",
    "print(max(history_v_sknn, key=lambda x: history_v_sknn[x]['hr']), metrics_v_sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_s_sknn, best_s_sknn_model, history_s_sknn, param_names_s_sknn = find_best_model(S_SKNN,\n",
    "                                                    knn_configs,\n",
    "                                                    data_dict,\n",
    "                                                    data_description,\n",
    "                                                    target_metric='hr')\n",
    "\n",
    "print(max(history_s_sknn, key=lambda x: history_s_sknn[x]['hr']), metrics_s_sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {\n",
    "    'SKNN':metrics_sknn,\n",
    "    'V-SKNN':metrics_v_sknn,\n",
    "    'S-SKNN':metrics_s_sknn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results = pd.DataFrame.from_dict(test_metrics).T\n",
    "\n",
    "pd.concat([simple_models_results, knn_results], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {\n",
    "    'SKNN':history_sknn,\n",
    "    'V-SKNN':history_v_sknn,\n",
    "    'S-SKNN':history_s_sknn\n",
    "}\n",
    "\n",
    "plot_histories(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra. Scalable weighted jaccard index, MinHash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All proofs and detailed derivations can be found in [Ioffe's original paper](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36928.pdf) and [Moulton's work](https://arxiv.org/pdf/1809.04052). There exists a python implementation available on [GitHub](https://github.com/ekzhu/datasketch). When feature vectors are very large and the Jaccard index needs to be computed frequently, using MinHash can significantly speed up the computations while maintaining comparable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import WeightedMinHashGenerator\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = [0.1, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.0, 0.4]\n",
    "v2 = [0.2, 0.4, 0.3, 0.8, 0.4, 0.7, 0.0, 0.9, 0.1, 0.0]\n",
    "\n",
    "min_sum = np.sum(np.minimum(v1, v2))\n",
    "max_sum = np.sum(np.maximum(v1, v2))\n",
    "true_jaccard = float(min_sum) / float(max_sum)\n",
    "\n",
    "wmg = WeightedMinHashGenerator(len(v1))\n",
    "wm1 = wmg.minhash(v1)\n",
    "wm2 = wmg.minhash(v2)\n",
    "print(\"Estimated Jaccard is\", wm1.jaccard(wm2))\n",
    "print(\"True Jaccard is\", true_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.random.rand(1000000)\n",
    "v2 = np.random.rand(1000000)\n",
    "wmg = WeightedMinHashGenerator(len(v1))\n",
    "\n",
    "wm1 = wmg.minhash(v1)\n",
    "wm2 = wmg.minhash(v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "\n",
    "min_sum = np.sum(np.minimum(v1, v2))\n",
    "max_sum = np.sum(np.maximum(v1, v2))\n",
    "float(min_sum) / float(max_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "wm1.jaccard(wm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = {}\n",
    "n_samples = 100\n",
    "sample_sizes = [10, 50, 100, 200, 500, 1000]\n",
    "for sample_size in sample_sizes:\n",
    "    if sample_size not in err:\n",
    "        err[sample_size] = []\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        v1 = np.random.rand(1000)\n",
    "        v2 = np.random.rand(1000)\n",
    "        wmg = WeightedMinHashGenerator(len(v1), seed=i, sample_size=sample_size)\n",
    "\n",
    "        wm1 = wmg.minhash(v1)\n",
    "        wm2 = wmg.minhash(v2)\n",
    "        \n",
    "        min_sum = np.sum(np.minimum(v1, v2))\n",
    "        max_sum = np.sum(np.maximum(v1, v2))\n",
    "        true = float(min_sum) / float(max_sum)\n",
    "        err[sample_size].append((true - wm1.jaccard(wm2)) / true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in err:\n",
    "    print(f'sample size: {key}, mean relative error: {np.mean(err[key]):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_err = {}\n",
    "for key in err:\n",
    "    plot_err[key] = np.std(err[key])\n",
    "x, y = zip(*plot_err.items())\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Sample size')\n",
    "plt.ylabel('Std of relative error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
