{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from polara import get_movielens_data\n",
    "from polara.lib.earlystopping import early_stopping_callback\n",
    "\n",
    "from scipy.sparse import diags, coo_matrix\n",
    "\n",
    "from source.dataprep.dataprep import split_data_global_timepoint, generate_interactions_matrix\n",
    "from source.evaluation.evaluation import topn_recommendations, model_evaluate, downvote_seen_items\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_movielens_data(include_time=True)\n",
    "data_description = {\n",
    "    'users':'userid',\n",
    "    'items':'movieid',\n",
    "    'feedback':'rating',\n",
    "    'timestamp':'timestamp'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testset_valid, holdout_valid, testset, holdout, data_index, data_description = split_data_global_timepoint(data=data, data_description=data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = {\n",
    "    'n_users':training.nunique()['userid'],\n",
    "    'n_items':training.nunique()['movieid'],\n",
    "    'users':'userid',\n",
    "    'items':'movieid',\n",
    "    'feedback':'rating',\n",
    "    'timestamp':'timestamp'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_histories(data, data_description):\n",
    "    histories = ...\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_{uij} = v^{U,I}_u \\cdot v^{I,U}_j + v^{I,L}_j \\cdot v^{L,I}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPMC:\n",
    "    def __init__(self, model_config) -> None:\n",
    "        self.model_config = model_config \n",
    "        # generator for interaction sampling\n",
    "        self.rng = np.random.default_rng(seed=model_config['seed'])\n",
    "        self.n_iters = model_config.get('epoch_iterations', 100000)\n",
    "        self.n_users = model_config['n_users']\n",
    "        self.n_items = model_config['n_items']\n",
    "        # random initialization of model's factors\n",
    "        self.sigma = model_config.get('sigma', 0.01)\n",
    "        self.V_LI = self.rng.normal(loc=0.0, scale=self.sigma, size=(model_config['n_items'], model_config['dim']))\n",
    "        self.V_IL = self.rng.normal(loc=0.0, scale=self.sigma, size=(model_config['n_items'], model_config['dim']))\n",
    "        self.V_IU = self.rng.normal(loc=0.0, scale=self.sigma, size=(model_config['n_items'], model_config['dim']))\n",
    "        self.V_UI = self.rng.normal(loc=0.0, scale=self.sigma, size=(model_config['n_users'], model_config['dim']))\n",
    "        \n",
    "    \n",
    "    def sigmoid(self, x, cutoff = 10.0):\n",
    "        sigmoid = ...\n",
    "\n",
    "        \n",
    "    def fit_partial(self, interactions, data_description, n_epochs=1, n_iters=None):\n",
    "        # fit the model. repeated calls to this method will\n",
    "        # cause training to resume from the current model state\n",
    "        histories = generate_histories(interactions, data_description)\n",
    "        iterations = n_iters if n_iters else len(interactions)\n",
    "        for epoch in range(n_epochs):\n",
    "            for _ in tqdm(range(iterations), desc='Training'):\n",
    "                # sample the triplet user, positive, negative\n",
    "                user = ...\n",
    "                pos_index = ...\n",
    "                pos = ...\n",
    "                neg = ...\n",
    "                \n",
    "                if pos_index == 0:\n",
    "                    # the first item in interaction history has no predecessor,\n",
    "                    # so we should calculate only MF part\n",
    "                    x_upos = ...\n",
    "                    x_uneg = ...\n",
    "                else:\n",
    "                    prev = histories[user][pos_index - 1]\n",
    "                    x_upos = ...\n",
    "                    x_uneg = ...\n",
    "                    \n",
    "                delta = 1.0 - self.sigmoid(x_upos - x_uneg)\n",
    "                \n",
    "                # update MF part\n",
    "                self.V_UI[user, :] += self.model_config['alpha'] * (\n",
    "                    delta * (self.V_IU[pos, :] - self.V_IU[neg, :])\n",
    "                    - self.model_config['L_UI'] * self.V_UI[user, :]\n",
    "                    )\n",
    "                self.V_IU[pos, :] += self.model_config['alpha'] * (\n",
    "                    delta * self.V_UI[user, :] - self.model_config['L_IU'] * self.V_IU[pos, :]\n",
    "                    )\n",
    "                self.V_IU[neg, :] += self.model_config['alpha'] * (\n",
    "                    -delta * self.V_UI[user, :] - self.model_config['L_IU'] * self.V_IU[neg, :]\n",
    "                    )\n",
    "                \n",
    "                if pos_index > 0:\n",
    "                    # if user's interaction is not the first interaction,\n",
    "                    # update MC part\n",
    "                    prev = histories[user][pos_index - 1]\n",
    "                    self.V_IL[pos, :] += self.model_config['alpha'] * (\n",
    "                        delta * self.V_LI[prev, :] - self.model_config['L_IL'] * self.V_IL[pos, :]\n",
    "                        )\n",
    "                    self.V_IL[neg, :] += self.model_config['alpha'] * (\n",
    "                        -delta * self.V_LI[prev, :] - self.model_config['L_IL'] * self.V_IL[neg, :]\n",
    "                        )\n",
    "                    self.V_LI[prev, :] += self.model_config['alpha'] * (\n",
    "                        delta * (self.V_IL[pos, :] - self.V_IL[neg, :])\n",
    "                        - self.model_config['L_LI'] * self.V_LI[prev, :]\n",
    "                        )\n",
    "                        \n",
    "    def folding_in(self, interactions, data_description, n_epochs=1, n_iters=None):\n",
    "        # this function allows to do folding-in of new users.\n",
    "        # it does the \"half-step\" of gd to update user embeddings\n",
    "        # without updating the item factors in MF and MC parts\n",
    "        histories = generate_histories(interactions, data_description)\n",
    "\n",
    "        # last interaction of each user to generate scores\n",
    "        last_interactions = np.array(\n",
    "            [histories[u][-1] for u in range(len(histories))]\n",
    "            )\n",
    "        iterations = n_iters if n_iters else self.n_iters\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for _ in tqdm(range(iterations), desc='Folding-in'):\n",
    "                # sample the triplet user, positive, negative\n",
    "                user = ...\n",
    "                pos_index = ...\n",
    "                pos = ...\n",
    "                neg = ...\n",
    "\n",
    "                if pos_index == 0:\n",
    "                    x_upos = ...\n",
    "                    x_uneg = ...\n",
    "                else:\n",
    "                    prev = histories[user][pos_index - 1]\n",
    "                    x_upos = ...\n",
    "                    x_uneg = ...\n",
    "                \n",
    "                delta = 1.0 - self.sigmoid(x_upos - x_uneg)\n",
    "                \n",
    "                # update only MF user embeddings\n",
    "                V_UI_warmstart[user, :] += self.model_config['alpha'] * (delta * (self.V_IU[pos, :] - self.V_IU[neg, :]) - self.model_config['L_UI'] * V_UI_warmstart[user, :])\n",
    "                \n",
    "        scores = ...\n",
    "        return scores\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpmc_config = {\n",
    "    'n_items':data_description['n_items'],\n",
    "    'n_users':data_description['n_users'],\n",
    "    'dim':128,\n",
    "    'seed':2024,\n",
    "    'alpha':0.05,\n",
    "    'L_UI':0.01,\n",
    "    'L_IU':0.01,\n",
    "    'L_IL':0.01,\n",
    "    'L_LI':0.01,\n",
    "    \n",
    "    'max_epochs':5\n",
    "}\n",
    "\n",
    "fpmc_model_test = FPMC(fpmc_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpmc_model_test.fit_partial(training, data_description, n_epochs=1, n_iters=10000)\n",
    "scores_val = fpmc_model_test.folding_in(testset_valid, data_description, n_epochs=1, n_iters=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpmc_config = {\n",
    "    'n_items':data_description['n_items'],\n",
    "    'n_users':data_description['n_users'],\n",
    "    'dim':128,\n",
    "    'seed':2024,\n",
    "    'alpha':0.05,\n",
    "    'L_UI':0.01,\n",
    "    'L_IU':0.01,\n",
    "    'L_IL':0.01,\n",
    "    'L_LI':0.01,\n",
    "    \n",
    "    'max_epochs':5\n",
    "}\n",
    "\n",
    "fpmc_model = FPMC(fpmc_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_norms = {\n",
    "    'UI':[],\n",
    "    'IU':[],\n",
    "    'LI':[],\n",
    "    'IL':[]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    fpmc_model.fit_partial(training, data_description, n_epochs=1, n_iters=100000)\n",
    "    scores_val = fpmc_model.folding_in(testset_valid, data_description, n_epochs=1, n_iters=100000)\n",
    "    downvote_seen_items(scores_val, testset_valid, data_description)\n",
    "    recs = topn_recommendations(scores_val)\n",
    "    print(f'epoch {epoch + 1}: {model_evaluate(recs, holdout_valid, data_description)}')\n",
    "\n",
    "    # save the factor norms for future analysis\n",
    "    factor_norms['UI'].append(np.linalg.norm(fpmc_model.V_UI, 'fro'))\n",
    "    factor_norms['IU'].append(np.linalg.norm(fpmc_model.V_IU, 'fro'))\n",
    "    factor_norms['IL'].append(np.linalg.norm(fpmc_model.V_IL, 'fro'))\n",
    "    factor_norms['LI'].append(np.linalg.norm(fpmc_model.V_LI, 'fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = fpmc_model.folding_in(testset, data_description, n_epochs=1, n_iters=100000)\n",
    "downvote_seen_items(scores, testset, data_description)\n",
    "recs = topn_recommendations(scores)\n",
    "print(model_evaluate(recs, holdout, data_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for factor in factor_norms.keys():\n",
    "    plt.plot(factor_norms[factor], label=factor)\n",
    "    \n",
    "plt.ylabel('Frobenius norm')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(Model_class, model_config, data, data_description, early_stop_config=None, iterator=None):\n",
    "    # the model\n",
    "    model = Model_class(model_config)\n",
    "    if iterator is None:\n",
    "        iterator = lambda x: x\n",
    "    # early stoppping configuration\n",
    "    es_config = check_early_stop_config(early_stop_config)\n",
    "    # training\n",
    "    for epoch in iterator(range(model_config['max_epochs'])):\n",
    "        try:\n",
    "            train_epoch(epoch, model, data, data_description, es_config)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "def check_early_stop_config(early_stop_config):\n",
    "    if early_stop_config is None:\n",
    "        early_stop_config = {}\n",
    "    try:\n",
    "        es_dict = dict(\n",
    "            early_stopper = early_stop_config['evaluation_callback'],\n",
    "            callback_interval = early_stop_config['callback_interval'],\n",
    "            warm_data = early_stop_config['warm_data'],\n",
    "            holdout = early_stop_config['holdout'],\n",
    "            stop_early = True\n",
    "        )\n",
    "    except KeyError:\n",
    "        es_dict = dict(stop_early = False)\n",
    "    return es_dict\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    epoch, model, train, data_description, es_config,\n",
    "):\n",
    "    model.fit_partial(\n",
    "        train,\n",
    "        data_description,\n",
    "        n_epochs=1,\n",
    "        n_iters=50000,\n",
    "    )\n",
    "    if es_config['stop_early'] and ((epoch+1) % es_config['callback_interval'] == 0):\n",
    "        # evaluate model and raise StopIteration if early stopping condition is met\n",
    "        es_config['early_stopper'](epoch, model, es_config['warm_data'], es_config['holdout'], data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warm_evaluator(model, warm_data, holdout, data_description, target_metric='hr', n_epochs=1, n_iters=50000):\n",
    "    scores = ...\n",
    "    downvote_seen_items(scores, warm_data, data_description)\n",
    "    recs = topn_recommendations(scores)\n",
    "    metrics = model_evaluate(recs, holdout, data_description)\n",
    "    return metrics[target_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_early_stop = early_stopping_callback(\n",
    "        warm_evaluator, max_fails=1, verbose=True\n",
    ")\n",
    "\n",
    "early_stop_config = dict(\n",
    "    evaluation_callback = try_early_stop,\n",
    "    callback_interval = 1, # break between consequent evaluation in epochs\n",
    "    holdout = holdout_valid,\n",
    "    warm_data = testset_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpmc_params = build_model(\n",
    "    FPMC,\n",
    "    fpmc_config,\n",
    "    training,\n",
    "    data_description,\n",
    "    early_stop_config=early_stop_config,\n",
    "    iterator=tqdm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Epochs: {try_early_stop.iter}, target metric: {try_early_stop.target}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOSSIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_u(j|S^u_{t-1}, S^u_{t-2}, S^u_{t-L})\\sim \\beta_j +  \\left[ \\frac{1}{|I_u^+ \\backslash \\{j\\}|} \\sum_{j' \\in I_u^+ \\backslash \\{j\\}} \\boldsymbol{P}_{j'}  + \\sum_{k=1}^L (\\eta_k+\\eta_k^u)\\boldsymbol{P}_{S_{t-k}^u}  , \\boldsymbol{Q}_j\\right]\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FOSSIL:\n",
    "    def __init__(self, model_config) -> None:\n",
    "        self.model_config = model_config\n",
    "        # generator for interaction sampling\n",
    "        self.rng = np.random.default_rng(seed=model_config['seed'])\n",
    "        # random initialization of model's factors\n",
    "        self.sigma = model_config.get('sigma', 0.01)\n",
    "        self.n_users = model_config['n_users']\n",
    "        self.n_items = model_config['n_items']\n",
    "        self.alpha = model_config['alpha']\n",
    "        self.lr = model_config['lr']\n",
    "        self.L = model_config['markov_order']\n",
    "        self.reg = model_config['regularization']\n",
    "        self.dim = model_config['dim']\n",
    "        \n",
    "        \n",
    "        self.P = self.rng.normal(loc=0.0, scale=self.sigma, size=(model_config['n_items'], model_config['dim']))\n",
    "        self.Q = self.rng.normal(loc=0.0, scale=self.sigma, size=(model_config['n_items'], model_config['dim']))\n",
    "        self.eta = self.rng.normal(loc=0.0, scale=self.sigma, size=(model_config['n_users'], model_config['markov_order']))\n",
    "        self.eta_bias = np.zeros(self.L)\n",
    "        self.bias = np.zeros(self.n_items)\n",
    "        \n",
    "        self.n_iters = model_config.get('epoch_iterations', 100000)\n",
    "\n",
    "    def sigmoid(self, x, cutoff = 10.0):\n",
    "        x_cutoff = max(min(cutoff, x), -cutoff)\n",
    "        return 1.0 / (1.0 + np.exp(-x_cutoff))\n",
    "\n",
    "    def compute_score(self, user_id, curr_hist, item):\n",
    "        length = min(self.L, len(curr_hist))\n",
    "        \n",
    "        long_term = ...\n",
    "\n",
    "        short_term = ...\n",
    "\n",
    "        return ...\n",
    "        \n",
    "    def fit_partial(self, interactions, data_description, n_epochs, n_iters=None):\n",
    "        # fit the model. repeated calls to this method will\n",
    "        # cause training to resume from the current model state\n",
    "        \n",
    "        histories = generate_histories(interactions, data_description)\n",
    "        iterations = n_iters if n_iters is not None else self.n_iters\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for _ in tqdm(range(iterations), desc='Training'):\n",
    "                \n",
    "                user = self.rng.integers(0, self.n_users)\n",
    "                user_history = histories[user]\n",
    "                pos_index = self.rng.integers(1, len(histories[user]))\n",
    "                curr_hist = user_history[:pos_index + 1]\n",
    "                neg = self.rng.choice(np.setdiff1d(np.arange(self.model_config['n_items']), user_history))\n",
    "\n",
    "                pos = curr_hist[-1]\n",
    "                curr_hist = curr_hist[:-1]\n",
    "                length = min(self.L, len(curr_hist))\n",
    "\n",
    "                long_term = ...\n",
    "                short_term = ...\n",
    "\n",
    "                x_pos = ...\n",
    "                x_neg = ...\n",
    "                \n",
    "                delta = 1.0 - self.sigmoid(x_pos - x_neg)\n",
    "                \n",
    "                # Gradients\n",
    "                V_upd = self.lr * ( delta * np.power(len(curr_hist), -self.alpha) * (self.Q[pos, :] - self.Q[neg, :]) - self.reg * self.P[curr_hist, :])\n",
    "                V_upd2 = self.lr * delta *  np.outer((self.eta_bias + self.eta[user, :])[:length], self.Q[pos, :] - self.Q[neg, :])\n",
    "                Q_pos_upd = self.lr * ( delta * (long_term + short_term) - self.reg * self.Q[pos, :])\n",
    "                Q_neg_upd = self.lr * ( -delta * (long_term + short_term) - self.reg * self.Q[neg, :])\n",
    "                bias_pos_upd = self.lr * (delta - self.reg * self.bias[pos])\n",
    "                bias_neg_upd = self.lr * (- delta - self.reg * self.bias[neg])\n",
    "                eta_bias_upd = self.lr * (delta * np.dot(self.P[curr_hist[:-length-1:-1], :], self.Q[pos, :] - self.Q[neg, :]) - self.reg * self.eta_bias[:length])\n",
    "                eta_upd = self.lr * (delta * np.dot(self.P[curr_hist[:-length-1:-1], :], self.Q[pos, :] - self.Q[neg, :]) - self.reg * self.eta[user, :length])\n",
    "\n",
    "                # Update\n",
    "                self.P[curr_hist, :] += V_upd\n",
    "                self.P[curr_hist[:-length-1:-1], :] += V_upd2\n",
    "                self.Q[pos, :] += Q_pos_upd\n",
    "                self.Q[neg, :] += Q_neg_upd\n",
    "                self.bias[pos] += bias_pos_upd\n",
    "                self.bias[neg] += bias_neg_upd\n",
    "                self.eta_bias[:length] += eta_bias_upd\n",
    "                self.eta[user, :length] += eta_upd\n",
    "                \n",
    "                        \n",
    "    def folding_in(self, interactions, data_description, n_epochs=1, n_iters = None):\n",
    "        # this function allows to do folding-in of new users.\n",
    "        # it does the \"half-step\" of gd to update user embeddings\n",
    "        # without updating the item factors in MF and MC parts\n",
    "        iterations = n_iters if n_iters is not None else self.n_iters\n",
    "        \n",
    "        histories = generate_histories(interactions, data_description)\n",
    "        eta_warmstart = self.rng.normal(\n",
    "            loc=0.0,\n",
    "            scale=self.sigma,\n",
    "            size=(len(histories),\n",
    "            self.model_config['markov_order']))\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for _ in tqdm(range(iterations), desc='Folding-in'):\n",
    "                \n",
    "                user = self.rng.integers(0, len(histories))\n",
    "                user_history = histories[user]\n",
    "                pos_index = self.rng.integers(1, len(histories[user]))\n",
    "                curr_hist = user_history[:pos_index + 1]\n",
    "                neg = self.rng.choice(\n",
    "                    np.setdiff1d(\n",
    "                        np.arange(self.model_config['n_items']),\n",
    "                        user_history))\n",
    "\n",
    "                pos = curr_hist[-1]\n",
    "                curr_hist = curr_hist[:-1]\n",
    "                length = min(self.L, len(curr_hist))\n",
    "\n",
    "                    \n",
    "                # Compute error\n",
    "                x_pos = ...\n",
    "                x_neg = ...\n",
    "                delta = self.sigmoid(x_pos - x_neg) # sigmoid of the error\n",
    "                \n",
    "                # Compute Update\n",
    "                eta_upd = self.lr * (delta * np.dot(self.P[curr_hist[:-length-1:-1], :], self.Q[pos, :] - self.Q[neg, :]) - self.reg * self.eta[user, :length])\n",
    "                eta_warmstart[user, :length] += eta_upd\n",
    "                \n",
    "        \n",
    "        scores = ...\n",
    "        \n",
    "        for user in range(len(histories)):\n",
    "            curr_hist = histories[user]\n",
    "            length = min(self.L, len(curr_hist))\n",
    "            \n",
    "            long_term = ...\n",
    "\n",
    "            short_term = ...\n",
    "            \n",
    "            scores = ...\n",
    "\n",
    "        return scores\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fossil_config = {\n",
    "    'n_items':data_description['n_items'],\n",
    "    'n_users':data_description['n_users'],\n",
    "    'seed':2024,\n",
    "    'dim':30,\n",
    "    'alpha':0.2,\n",
    "    'lr':0.01,\n",
    "    'markov_order':2,\n",
    "    'regularization':0.1,\n",
    "}\n",
    "\n",
    "fossil_model_test = FOSSIL(fossil_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fossil_model_test.fit_partial(training, data_description, n_epochs=1, n_iters=10000)\n",
    "scores_val = fossil_model_test.folding_in(testset_valid, data_description, n_epochs=1, n_iters=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fossil_config = {\n",
    "    'n_items':data_description['n_items'],\n",
    "    'n_users':data_description['n_users'],\n",
    "    'seed':2024,\n",
    "    'dim':30,\n",
    "    'alpha':0.2,\n",
    "    'lr':0.01,\n",
    "    'markov_order':2,\n",
    "    'regularization':0.1,\n",
    "}\n",
    "\n",
    "fossil_model = FOSSIL(fossil_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    fossil_model.fit_partial(training, data_description, n_epochs=1, n_iters=300000)\n",
    "    scores_val = fossil_model.folding_in(testset_valid, data_description, n_epochs=1, n_iters=100000)\n",
    "    downvote_seen_items(scores_val, testset_valid, data_description)\n",
    "    recs = topn_recommendations(scores_val)\n",
    "    print(f'epoch {epoch + 1}: {model_evaluate(recs, holdout_valid, data_description)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = fossil_model.folding_in(testset, data_description, n_epochs=1, n_iters=100000)\n",
    "downvote_seen_items(scores_test, testset, data_description)\n",
    "recs = topn_recommendations(scores_test)\n",
    "print(model_evaluate(recs, holdout, data_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history_lengths = training.groupby(data_description['users']).size().values\n",
    "fossil_user_factor = np.linalg.norm(fossil_model.eta, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_history_lengths, fossil_user_factor)\n",
    "plt.ylabel('Norm of user vector')\n",
    "plt.xlabel('Length of user history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeqMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathcal{L}(\\bold{P}, \\bold{Q}) = \\frac{1}{2}\\sum_{u}\\|\\bold{r}(\\bold{Q}, \\bold{p}_u)-\\bold{a}_u\\|^2_{\\bold{C}_u} + \\frac{\\lambda}{2}\\left( \\sum_{u}\\|\\bold{p}_u\\|_2^2+\\|\\bold{Q}\\|_F^2 \\right) \\rightarrow \\min_{\\{\\bold{Q}, \\bold{p}_u\\}}\n",
    "$$\n",
    "\n",
    "The loss is almost identical to the regular MF. The sequential setting of the model is hidden at the inference step:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\bold{r}(\\bold{Q}, \\bold{p}_u) = \\bold{Q}\\bold{p}_u + \\text{diag}(\\bold{S}_u\\bold{Q}\\bold{Q}^\\top)\n",
    "\\end{equation}\n",
    "\n",
    "where $S_u$ is the item transition matrix of user $u$. It encodes how likely the transition from item $j$ to item $i$ to occur (recall sequential rules):\n",
    "\n",
    "$$\n",
    "    (\\bold{S}_u)_{ij} = \\frac{\\sum_{k=2}^{|\\mathcal{H}_u^{(t)}|} \\bold{1}[j \\rightarrow i] \\bold{1}[i_{k-1}^u=j] \\bold{1}[i_k^u=i]}{\\sum_{k=2}^{|\\mathcal{H}_u^{(t)}|} \\bold{1}[i_{k-1}^u=i]}\n",
    "$$\n",
    "\n",
    "$\\bold{C}_u$ is a diagonal matrix, containing weights based on item popularity ($N$ is number of items):\n",
    "$$\n",
    "    (\\bold{C}_u)_{i} = \\frac{d_{ui}^\\gamma + \\alpha}{\\sum_{j}d_{ui}^\\gamma + \\alpha N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up model's inference, the equation (1) can be simplified:\n",
    "\n",
    "$$\n",
    "\\bold{r}(\\bold{Q}, \\bold{p}_u) = \\bold{Q}\\bold{p}_u + \\text{diag}(\\bold{S}_u\\bold{Q}\\bold{Q}^\\top) \\approx \\bold{Q}\\bold{p}_u + \\bold{Q}\\sum_{i\\in s_u}\\bold{q}_i = \\bold{Q}\\bold{p}_u + h_u(\\bold{Q})\n",
    "$$\n",
    "\n",
    "where the summation in second term goes over the last $l$ items of user's interaction history: $s_u=(i_1, i_2, ..., i_l)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bold{p}_u = (\\bold{Q}^\\top \\bold{C}_u\\bold{Q}+\\lambda \\bold{I})^{-1}\\bold{Q}^\\top \\bold{C}_u (a_u-h_u(\\bold{Q})) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\bold{Q}} = \\lambda \\bold{Q} + \\sum_{u = 1}^M\\bold{F}(u)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bold{F}(u) = \\bold{D}_u\\bold{e}_N \\bold{p}_u^\\top + (\\bold{D}_u \\bold{S}_u + \\bold{S}_u^\\top\\bold{D}_u)\\bold{Q}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bold{D}_u = \\text{diag}(\\bold{C}_u(\\bold{r}(\\bold{Q}, \\bold{p}_u) - \\bold{a}_u))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqMF:\n",
    "    def __init__(self, model_config) -> None:\n",
    "        self.model_config = model_config\n",
    "        self.last_n = model_config['last_n']\n",
    "        self.gamma = model_config['gamma']\n",
    "        self.beta = model_config['beta']\n",
    "        self.lambd = model_config['lambda']\n",
    "        self.n_items = model_config['n_items']\n",
    "        self.dim = model_config['dim']\n",
    "        sigma = model_config.get('sigma', 0.01)\n",
    "        self.P = sigma * np.random.randn(model_config['n_users'], model_config['dim'])\n",
    "        self.Q = sigma * np.random.randn(model_config['n_items'], model_config['dim'])\n",
    "        self.S = []\n",
    "        self.C = []\n",
    "        \n",
    "    \n",
    "    def h(self, user, history):\n",
    "        last_n_items = ...\n",
    "        return ...\n",
    "    \n",
    "    def precompute(self, data, data_description):\n",
    "        '''\n",
    "        Builds C_u and S_u matrices for each user.\n",
    "        C_u is weighted frequency of user's item interaction\n",
    "        S_u is the item transition matrix for the user\n",
    "        '''\n",
    "        # for u in tqdm(range(len(histories)), desc='Build C&S'):\n",
    "        histories = generate_histories(data, data_description)\n",
    "        interactions = generate_interactions_matrix(data, data_description)\n",
    "        \n",
    "        for u in range(len(histories)):\n",
    "            weights = interactions[u, :].A.squeeze() ** self.gamma\n",
    "            C_u = diags(weights / weights.sum())\n",
    "            self.C.append(C_u)\n",
    "\n",
    "            rules = {}\n",
    "            denominator = np.zeros((self.n_items))\n",
    "            for i in range(1, len(histories[u])):\n",
    "                if (histories[u][i - 1], histories[u][i]) not in rules:\n",
    "                    rules[(histories[u][i - 1], histories[u][i])] = 0\n",
    "                rules[(histories[u][i - 1], histories[u][i])] += 1\n",
    "                denominator[histories[u][i - 1]] += 1\n",
    "                \n",
    "            # create a sparse matrix\n",
    "            items, values = zip(*rules.items())\n",
    "            i1, i2 = zip(*items)\n",
    "            matrix_shape = (data_description['n_items'], data_description['n_items'])\n",
    "            S_u = coo_matrix((values, (list(i1), list(i2))), shape=matrix_shape).tocsr()\n",
    "                \n",
    "            S_u = S_u @ diags(np.divide(1.0, denominator, where=(denominator!=0), out=denominator))\n",
    "\n",
    "            self.S.append(S_u)\n",
    "\n",
    "    def fit(self, data, data_description, n_epochs):\n",
    "        # !!! hybrid optimization scheme !!!\n",
    "        # user factors are updated using ALS\n",
    "        # item factors are updated using GD\n",
    "\n",
    "        if len(self.S) == 0:\n",
    "            self.precompute(data, data_description)\n",
    "            \n",
    "        interactions = generate_interactions_matrix(data, data_description)\n",
    "        histories = generate_histories(data, data_description)\n",
    "                \n",
    "        for epoch in range(n_epochs):\n",
    "            # updating user factors through ALS scheme\n",
    "            # for u in tqdm(range(len(histories)), desc='update user'):\n",
    "            for u in tqdm(range(len(histories)), desc='Updating user factors'):\n",
    "                self.P[u, :] = ...\n",
    "                \n",
    "            # updating item factors via GD\n",
    "            grad = self.lambd * self.Q\n",
    "            u_ids = np.random.permutation(len(histories))\n",
    "            for u in tqdm(u_ids, desc='Updating item factors'):\n",
    "                D_u = ...\n",
    "                grad += ...\n",
    "            self.Q -= self.beta * grad\n",
    "    \n",
    "    def folding_in(self, data, data_description, n_epochs=None):\n",
    "        C_warm = []\n",
    "        warm_interactions = generate_interactions_matrix(data, data_description, rebase_users=True)\n",
    "        warm_histories = generate_histories(data, data_description)\n",
    "        n_warm_users = warm_interactions.shape[0]\n",
    "        \n",
    "        \n",
    "        for u in range(n_warm_users):\n",
    "            weights = ...\n",
    "            C_u = ...\n",
    "            C_warm.append(C_u)\n",
    "            \n",
    "        \n",
    "        P_warm = np.zeros((n_warm_users, self.dim))\n",
    "        \n",
    "        for u in range(n_warm_users):\n",
    "            P_warm[u, :] = ...\n",
    "\n",
    "        scores = ...\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqmf_config = {\n",
    "    'n_items':data_description['n_items'],\n",
    "    'n_users':data_description['n_users'],\n",
    "    'dim':16,\n",
    "    'last_n':3,\n",
    "    'beta':0.01,\n",
    "    'lambda':0.5,\n",
    "    'gamma':0.5,\n",
    "}\n",
    "\n",
    "\n",
    "seqmf_model = SeqMF(seqmf_config)\n",
    "\n",
    "factor_norms = {\n",
    "    'P':[],\n",
    "    'Q':[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    seqmf_model.fit(training, data_description, 1)\n",
    "    \n",
    "    factor_norms['P'].append(np.linalg.norm(seqmf_model.P, 'fro'))\n",
    "    factor_norms['Q'].append(np.linalg.norm(seqmf_model.Q, 'fro'))\n",
    "    \n",
    "    scores_val = seqmf_model.folding_in(testset_valid, data_description)\n",
    "    downvote_seen_items(scores_val, testset_valid, data_description)\n",
    "    recs = topn_recommendations(scores_val)\n",
    "    print(f'epoch {epoch + 1}: {model_evaluate(recs, holdout_valid, data_description)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = seqmf_model.folding_in(testset, data_description)\n",
    "downvote_seen_items(scores, testset, data_description)\n",
    "recs = topn_recommendations(scores)\n",
    "print(model_evaluate(recs, holdout, data_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for factor in factor_norms.keys():\n",
    "    plt.plot(factor_norms[factor], label=factor)\n",
    "    \n",
    "plt.ylabel('Frobenius norm')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
