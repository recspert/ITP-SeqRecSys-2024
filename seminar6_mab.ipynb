{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed bandits (MAB) and the cold-start scenario. Comparison with hybrid recommenders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "username = 'recspert'\n",
    "repo = 'ITP-SeqRecSys-2024'\n",
    "\n",
    "# remove local directory if it already exists\n",
    "if os.path.isdir(repo):\n",
    "    !rm -rf {repo}\n",
    "\n",
    "!git clone https://github.com/{username}/{repo}.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import reindex\n",
    "\n",
    "%cd {repo}\n",
    "from source.dataprep.dataprep import transform_indices, generate_interactions_matrix\n",
    "from source.evaluation.evaluation import topn_recommendations\n",
    "%cd -\n",
    "\n",
    "from lightfm import LightFM\n",
    "\n",
    "from scipy.sparse import csr_matrix, identity, hstack\n",
    "import matplotlib.pyplot as plt\n",
    "import os, time\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://files.grouplens.org/datasets/movielens/ml-1m.zip --output ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './ml-1m/'\n",
    "data = pd.read_csv(os.path.join(data_path, 'ratings.dat'), sep='::', names=['userid', 'movieid', 'rating', 'timestamp'], header=None, engine='python')\n",
    "data['rating'] = (data['rating'] >= 4).astype(int)\n",
    "user_data = pd.read_csv(os.path.join(data_path, 'users.dat'), sep='::', names=['userid', 'gender', 'age', 'occupation', 'zip-code'], header=None, engine='python')\n",
    "item_data = pd.read_csv(os.path.join(data_path, 'movies.dat'), sep='::', names=['movieid', 'title', 'genres'], header=None, engine='python', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items = data['movieid'].value_counts().index[:100]\n",
    "data = data[data['movieid'].isin(top_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = {\n",
    "    'items':'movieid',\n",
    "    'users':'userid',\n",
    "    'feedback':'rating',\n",
    "    'timestamp':'timestamp'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = np.random.choice(data[data_description['users']].unique(), 500, replace=False)\n",
    "\n",
    "test_data_ = data.query('userid in @test_users')\n",
    "train_data_ = data.query('userid not in @test_users')\n",
    "\n",
    "training, data_index = transform_indices(train_data_.copy(), 'userid', 'movieid')\n",
    "\n",
    "genres_split = item_data['genres'].str.get_dummies(sep='|')\n",
    "item_features_ = pd.concat([item_data['movieid'], genres_split], axis=1)\n",
    "item_features_train = reindex(item_features_, data_index['items']).set_index(data_description['items'])\n",
    "\n",
    "gender = pd.get_dummies(user_data['gender']).astype(int)\n",
    "occupation = pd.get_dummies(user_data['occupation'], prefix='occupation').astype(int)\n",
    "user_features_ = pd.concat([user_data['userid'], gender, occupation], axis=1)\n",
    "user_features_train_ = user_features_[user_features_[data_description['users']].isin(train_data_[data_description['users']].unique())]\n",
    "user_features_test_ = user_features_[user_features_[data_description['users']].isin(test_users)]\n",
    "\n",
    "user_idx = pd.Index(\n",
    "        test_data_[data_description['users']].unique(),\n",
    "        dtype=data_index['users'].dtype,\n",
    "        name=data_description['users'],\n",
    "        )\n",
    "user_features_train = reindex(user_features_train_, data_index['users']).set_index(data_description['users'])\n",
    "user_features_test = reindex(user_features_test_, user_idx).set_index(data_description['users'])\n",
    "\n",
    "\n",
    "testset = reindex(test_data_, [user_idx, data_index['items']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[mab2rec](https://github.com/fidelity/mab2rec), [mabwiser](https://github.com/fidelity/mabwiser)\n",
    "\n",
    "[openbandit](https://github.com/st-tech/zr-obp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon greedy\n",
    "\n",
    "With probability $\\varepsilon$ do eploration (pull a random arm), with probability $1 - \\varepsilon$ do eploitation (choose the best arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class epsGreedy:\n",
    "    def __init__(self, config) -> None:\n",
    "        self.n_arms = config['n_arms']\n",
    "        self.epsilon = config['epsilon']\n",
    "        self.seed = config['seed']\n",
    "        self.rng = np.random.default_rng(seed=config['seed'])\n",
    "        self.Q = np.zeros(config['n_arms'])\n",
    "        self.click = np.zeros(config['n_arms'])\n",
    "        \n",
    "    def train(self, data, data_description, features):\n",
    "        ...\n",
    "    \n",
    "    def choose_arm(self, context):\n",
    "        ...\n",
    "        return arm\n",
    "    \n",
    "    def update(self, arm, context, reward, update):\n",
    "        ...\n",
    "        \n",
    "    def compute_probs(self, context):\n",
    "        scores = np.tile(self.Q, (context.shape[0], 1))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinUCB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_a = A_a^{-1}b_a\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{t,a} = \\theta_a^\\top x_{t,a} + \\alpha \\sqrt{x_{t,a}^\\top A_a^{-1} x_{t,a}}\n",
    "$$\n",
    "\n",
    "Choose arm with highest $p_{t, a}$, observe reward $r_t$.\n",
    "\n",
    "$$\n",
    "A_a \\leftarrow A_a + x_{t,a}x_{t,a}^\\top\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_a \\leftarrow b_a + r_tx_{t,a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinUCB:\n",
    "    def __init__(self, config):\n",
    "        self.n_arms = config['n_arms']\n",
    "        self.d = config['context_dim']\n",
    "        self.alpha = config['alpha']\n",
    "        self.seed = config['seed']\n",
    "        self.rng = np.random.default_rng(seed=config['seed'])\n",
    "        \n",
    "        self.A = np.array([np.eye(config['context_dim']) for _ in range(config['n_arms'])])\n",
    "        self.b = np.array([np.zeros(config['context_dim']) for _ in range(config['n_arms'])])\n",
    "        self.A_inv = np.array([np.eye(config['context_dim']) for _ in range(config['n_arms'])])\n",
    "        self.theta = np.zeros((config['n_arms'], config['context_dim']))\n",
    "\n",
    "    def train(self, data, data_description, features):\n",
    "        occurences = data.groupby(data_description['items']).size()\n",
    "        positives = data.groupby(data_description['items'])[data_description['feedback']].sum()\n",
    "        \n",
    "        for item in occurences.index:\n",
    "            self.A[item] += np.outer(features.loc[item].values, features.loc[item].values) * occurences.loc[item]\n",
    "            self.b[item] += features.loc[item].values * positives.loc[item]\n",
    "            \n",
    "        self.A_inv = np.linalg.inv(self.A)\n",
    "        self.theta = np.einsum('ijk,ik->ij', self.A_inv, self.b, optimize=True)\n",
    "        \n",
    "\n",
    "    def choose_arm(self, features):\n",
    "        n_items = self.A.shape[0]\n",
    "        probs = np.zeros(n_items)\n",
    "\n",
    "        probs = np.einsum('ij,ij->i', self.theta, features.values, optimize=True) + self.alpha * np.sqrt(\n",
    "            np.einsum('ijk,ij,ik->i', self.A_inv, features.values, features.values, optimize=True)\n",
    "        )\n",
    "\n",
    "        chosen_arm = np.argmax(probs)\n",
    "        \n",
    "        return chosen_arm\n",
    "    \n",
    "    def update(self, arm, features, reward, update=True):\n",
    "        ...\n",
    "            \n",
    "    def compute_probs(self, features):\n",
    "        n_items = self.A.shape[0]\n",
    "        probs = np.zeros(n_items)\n",
    "\n",
    "        probs = np.einsum('ij,ij->i', self.theta, features.values, optimize=True) + self.alpha * np.sqrt(\n",
    "            np.einsum('ijk,ij,ik->i', self.A_inv, features.values, features.values, optimize=True)\n",
    "        )\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_evaluate(Bandit, bandit_cfg, data, data_description, batchsize=100):\n",
    "    bandit = Bandit(bandit_cfg)\n",
    "    bandit.train(data['train'], data_description, data['features'])\n",
    "\n",
    "    # stream of events\n",
    "    current_reward = 0\n",
    "    hr = 0\n",
    "    unique = set()\n",
    "    \n",
    "    step = 1\n",
    "    rewards_history = []\n",
    "    for i in range(len(data['interactions'])):\n",
    "        item = data['interactions'].iloc[i][data_description['items']]\n",
    "        reward = data['interactions'].iloc[i][data_description['feedback']]\n",
    "\n",
    "        arm = bandit.choose_arm(data['features'])\n",
    "        \n",
    "        if arm == item:\n",
    "            current_reward += reward\n",
    "            hr += 1\n",
    "            unique.add(item)\n",
    "\n",
    "        rewards_history.append(current_reward / step)\n",
    "        step += 1\n",
    "        \n",
    "        bandit.update(arm, data['features'], reward, update=(i + 1) % batchsize == 0)\n",
    "    print('hr: ', hr / step, 'cov: ', len(unique) / bandit.n_arms)\n",
    "    return bandit, rewards_history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_data = {\n",
    "    'train':training,\n",
    "    'interactions':testset.sort_values(by=data_description['timestamp']),\n",
    "    'features':item_features_train,\n",
    "    'user_features':user_features_train,\n",
    "    'user_features_test':user_features_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb_results = {}\n",
    "for alpha in [-0.5, 0.1, 1.0, 5.0]:\n",
    "    ucb_cfg = {\n",
    "        'n_arms':bandit_data['interactions'].nunique()[data_description['items']],\n",
    "        'context_dim':bandit_data['features'].shape[1],\n",
    "        'alpha':alpha,\n",
    "        'seed':2024\n",
    "    }\n",
    "    ucb, ucb_rewards = stream_evaluate(LinUCB, ucb_cfg, bandit_data, data_description, batchsize=100)\n",
    "    plt.plot(ucb_rewards, label=alpha)\n",
    "    ucb_results[alpha] = ucb_rewards\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_results = {}\n",
    "for epsilon in [0.1, 0.5, 0.9]:\n",
    "    eps_cfg = {\n",
    "        'n_arms':bandit_data['interactions'].nunique()[data_description['items']],\n",
    "        'epsilon':epsilon,\n",
    "        'seed':2024\n",
    "    }\n",
    "    eps, eps_rewards = stream_evaluate(epsGreedy, eps_cfg, bandit_data, data_description, batchsize=100)\n",
    "    plt.plot(eps_rewards, label=epsilon)\n",
    "    eps_results[epsilon] = eps_rewards\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Code](https://github.com/lyst/lightfm/tree/master), [paper](https://arxiv.org/pdf/1507.08439)\n",
    "\n",
    "$$\n",
    "q_u = \\sum_{j\\in f_u}e^U_j, \\ \\ \\ b_u = \\sum_{j\\in f_u}b^U_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_i = \\sum_{j\\in f_i}e^I_j, \\ \\ \\ b_i = \\sum_{j\\in f_i}b^I_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{ui} = \\sigma(q_u^\\top p_i + b_u + b_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate_coldstart(recommended_items, holdout, holdout_description, topn=10):\n",
    "    itemid = holdout_description['items']\n",
    "    holdout_items = holdout[itemid].values\n",
    "    assert recommended_items.shape[0] == len(holdout[itemid].nunique())\n",
    "    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n",
    "    # HR calculation\n",
    "    hr = np.mean(hits_mask.any(axis=1))\n",
    "    # MRR calculation\n",
    "    n_test_items = recommended_items.shape[0]\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    mrr = np.sum(1 / hit_rank) / n_test_items\n",
    "    # coverage calculation\n",
    "    n_users = holdout_description['n_users']\n",
    "    cov = np.unique(recommended_items).size / n_users\n",
    "    return hr, mrr, cov\n",
    "\n",
    "\n",
    "def check_early_stop_config_coldstart(early_stop_config):\n",
    "    \"\"\"\n",
    "    Validates the early stop configuration and returns a config dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    early_stop_config : dict, optional\n",
    "        Dictionary containing the early stop configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    es_dict : dict\n",
    "        Dictionary containing the early stop configuration, or a dictionary\n",
    "        with 'stop_early' set to False if no valid configuration is provided.\n",
    "    \"\"\"\n",
    "    if early_stop_config is None:\n",
    "        early_stop_config = {}\n",
    "    try:\n",
    "        es_dict = {\n",
    "            'early_stopper': early_stop_config['evaluation_callback'],\n",
    "            'callback_interval': early_stop_config['callback_interval'],\n",
    "            'item_features': early_stop_config['item_features'],\n",
    "            'train': early_stop_config['train'],\n",
    "            'holdout': early_stop_config['holdout'],\n",
    "            'target_metric': early_stop_config['target_metric'],\n",
    "            'stop_early': True\n",
    "        }\n",
    "    except KeyError: # config is invalid, doesn't contain required keys\n",
    "        es_dict = {'stop_early': False} # disable early stopping\n",
    "    return es_dict\n",
    "\n",
    "\n",
    "def lfm_best_model_search(data, data_description, config):\n",
    "    data_descr_lfm = dict(\n",
    "        users = data_description['users'],\n",
    "        items = data_description['items'],\n",
    "        feedback = data_description['feedback'],\n",
    "        n_users = data['interactions'].shape[0],\n",
    "        n_items = data['interactions'].shape[1],\n",
    "        user_features = hstack(\n",
    "            [\n",
    "                identity(data['interactions'].shape[0], dtype=data['item_features'].dtype, format='csr'),\n",
    "                data['user_features']\n",
    "                ]).tocsr(),\n",
    "        item_features = hstack(\n",
    "            [\n",
    "                identity(data['interactions'].shape[1], dtype=data['user_features'].dtype, format='csr'),\n",
    "                data['item_features']\n",
    "                ]).tocsr(),\n",
    "    )\n",
    "\n",
    "    lfm_params = build_lfm_model(\n",
    "        config,\n",
    "        data['interactions'],\n",
    "        data_descr_lfm,\n",
    "        early_stop_config = None\n",
    "    )\n",
    "    return lfm_params\n",
    "\n",
    "def build_lfm_model(config, data, data_description, early_stop_config=None, iterator=None):\n",
    "\n",
    "    model = LightFM(\n",
    "        no_components = config['no_components'],\n",
    "        loss = config['loss'],\n",
    "        learning_schedule = config['learning_schedule'],\n",
    "        # learning_rate=\n",
    "        user_alpha = config['user_alpha'],\n",
    "        item_alpha = config['item_alpha'],\n",
    "        max_sampled = config['max_sampled'],\n",
    "        # random_state = \n",
    "    )\n",
    "    # early stoppping configuration\n",
    "    es_config = check_early_stop_config_coldstart(early_stop_config)\n",
    "\n",
    "    # training\n",
    "    if iterator is None:\n",
    "        iterator = lambda x: x\n",
    "    for epoch in iterator(range(config['max_epochs'])):\n",
    "        try:\n",
    "            train_lfm_epoch(epoch, model, data, data_description, es_config)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return model\n",
    "\n",
    "def train_lfm_epoch(\n",
    "    epoch, model, train, data_description, es_config,\n",
    "):\n",
    "\n",
    "    model.fit_partial(\n",
    "        train,\n",
    "        user_features = data_description['user_features'],\n",
    "        item_features = data_description['item_features'],\n",
    "        epochs = 1,\n",
    "        num_threads=80\n",
    "    )\n",
    "    if not es_config['stop_early']:\n",
    "        return\n",
    "    metrics_check_interval = es_config['callback_interval']\n",
    "    if (epoch+1) % metrics_check_interval == 0:\n",
    "        # evaluate model and raise StopIteration if early stopping condition is met\n",
    "        \n",
    "        early_stopper_call = es_config['early_stopper']\n",
    "        early_stopper_call(epoch, model, es_config['user_features'], es_config['item_features'], es_config['holdout'], data_description, es_config['target_metric'])\n",
    "        \n",
    "        \n",
    "def lightfm_scoring(model, user_features, item_features, data_description):\n",
    "    \"\"\"\n",
    "    A standard scoring function adopted for use with LightFM in the item cold-start settings.\n",
    "    It returns a 2D item-user array (i.e., a transposed matrix of interactions) corresponding\n",
    "    to the predicted scores of user relevance to cold items.\n",
    "    \"\"\"    \n",
    "    user_biases, user_embeddings = model.get_user_representations(features=user_features)\n",
    "    item_biases, item_embeddings = model.get_item_representations(features=item_features)\n",
    "    scores = user_embeddings @ item_embeddings.T  + item_biases\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def lfm_evaluator(model, user_features, item_features, holdout, data_description, target_metric):\n",
    "    \"\"\"\n",
    "    Helper function to run within an evaluation callback.\n",
    "    \n",
    "    Intended usage:\n",
    "    - in the early stopping setting for tuning based on a `target_metric`.\n",
    "    \"\"\"\n",
    "    holdout_users = holdout[data_description['users']].values\n",
    "\n",
    "    lfm_scores = lightfm_scoring(model, user_features[holdout_users, :], item_features, data_description)\n",
    "    lfm_recs = topn_recommendations(lfm_scores)\n",
    "    metrics = model_evaluate_coldstart(lfm_recs, holdout, data_description)\n",
    "    return metrics[target_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description['n_users'] = training.nunique()[data_description['users']]\n",
    "data_description['n_items'] = training.nunique()[data_description['items']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_matrix = generate_interactions_matrix(training, data_description)\n",
    "\n",
    "lfm = lfm_best_model_search(\n",
    "    {\n",
    "        'interactions':interactions_matrix,\n",
    "        'item_features':csr_matrix(item_features_train.values),\n",
    "        'user_features':csr_matrix(user_features_train.values),\n",
    "        'n_users':data_description['n_users'],\n",
    "        'n_items':data_description['n_items'],\n",
    "    },\n",
    "    data_description,\n",
    "    dict(\n",
    "    no_components = 64,\n",
    "    loss = 'warp',\n",
    "    max_sampled = 3,\n",
    "    max_epochs = 10,\n",
    "    learning_schedule = 'adagrad',\n",
    "    user_alpha = 1e-5,\n",
    "    item_alpha = 1e-5,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_evaluate_lfm(lfm, data, data_description, batchsize=100):\n",
    "    # stream of events\n",
    "    current_reward = 0\n",
    "    hr = 0\n",
    "    unique = set()\n",
    "    \n",
    "    step = 1\n",
    "    rewards_history = []\n",
    "    interactions_matrix = generate_interactions_matrix(data['train'], data_description)\n",
    "    item_features_test = hstack(\n",
    "        [\n",
    "        identity(interactions_matrix.shape[1], format='csr'),\n",
    "        csr_matrix(data['features'].values),\n",
    "        ]).tocsr()\n",
    "\n",
    "    for i in range(len(data['interactions'])):\n",
    "        item = data['interactions'].iloc[i][data_description['items']]\n",
    "        user = data['interactions'].iloc[i][data_description['users']]\n",
    "        reward = data['interactions'].iloc[i][data_description['feedback']]\n",
    "        \n",
    "        user_features = hstack(\n",
    "        [\n",
    "            csr_matrix(([], ([], [])), shape=(1, interactions_matrix.shape[0])),\n",
    "            csr_matrix(data['user_features_test'].values[user, :]),\n",
    "        ]).tocsr()\n",
    "\n",
    "        arm = np.argmax(lightfm_scoring(lfm, user_features, item_features_test, data_description))\n",
    "        \n",
    "        if arm == item:\n",
    "            current_reward += reward\n",
    "            hr += 1\n",
    "            unique.add(item)\n",
    "\n",
    "        rewards_history.append(current_reward / step)\n",
    "        step += 1\n",
    "        \n",
    "    print('hr: ', hr / step, 'cov: ', len(unique) / data_description['n_items'])\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_rewards = stream_evaluate_lfm(lfm, bandit_data, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lfm_rewards, label='LightFM', c='black')\n",
    "\n",
    "for alpha in ucb_results:\n",
    "    plt.plot(ucb_results[alpha], label=f'UCB_{alpha}')\n",
    "\n",
    "for epsilon in eps_results:\n",
    "    plt.plot(eps_results[epsilon], label=f'EPS_{epsilon}', ls='dashed')\n",
    "    \n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.ylim(0.0, 0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{V}^{\\text{ips}}_e = \\frac{1}{n} \\sum_{i = 1}^n \\frac{\\pi_e(a_i|x_i)}{\\pi_0(a_i|x_i)}r_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_offpolicy_ips(Bandit, bandit_cfg, data, data_description, batchsize=10000):\n",
    "    bandit = Bandit(bandit_cfg)\n",
    "    bandit.train(data['train'], data_description, data['features'])\n",
    "\n",
    "    # stream of events\n",
    "    v_ips = 0\n",
    "    step = 1\n",
    "    ips_scores = []\n",
    "    for i in range(len(data['interactions'])):\n",
    "        if (i + 1) % batchsize == 0:\n",
    "            ips_scores.append(v_ips / step)\n",
    "            v_ips = 0\n",
    "            step = 1\n",
    "        item = data['interactions'].iloc[i][data_description['items']]\n",
    "        reward = data['interactions'].iloc[i][data_description['feedback']]\n",
    "        \n",
    "        v_ips += reward * bandit.compute_probs(data['features'])[item] / (1 / bandit.n_arms)\n",
    "\n",
    "        step += 1\n",
    "    return bandit, ips_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb_cfg = {\n",
    "    'n_arms':bandit_data['interactions'].nunique()[data_description['items']],\n",
    "    'context_dim':bandit_data['features'].shape[1],\n",
    "    'alpha':1.0,\n",
    "    'seed':2024\n",
    "}\n",
    "\n",
    "ucb, ips_scores_ucb = train_offpolicy_ips(LinUCB, ucb_cfg, bandit_data, data_description, batchsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.mean(ips_scores_ucb)} +- {np.std(ips_scores_ucb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_cfg = {\n",
    "    'n_arms':bandit_data['interactions'].nunique()[data_description['items']],\n",
    "    'epsilon':0.5,\n",
    "    'seed':2024\n",
    "}\n",
    "eps, ips_scores_eps = train_offpolicy_ips(epsGreedy, eps_cfg, bandit_data, data_description, batchsize=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.mean(ips_scores_eps)} +- {np.std(ips_scores_eps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offpolicy_ips(lfm, data, data_description, batchsize=100):\n",
    "    # stream of events\n",
    "    v_ips = 0\n",
    "    step = 1\n",
    "    ips_scores = []\n",
    "\n",
    "    interactions_matrix = generate_interactions_matrix(data['train'], data_description)\n",
    "    item_features_test = hstack(\n",
    "        [\n",
    "        identity(interactions_matrix.shape[1], format='csr'),\n",
    "        csr_matrix(data['features'].values),\n",
    "        ]).tocsr()\n",
    "\n",
    "    for i in range(len(data['interactions'])):\n",
    "        if (i + 1) % batchsize == 0:\n",
    "            ips_scores.append(v_ips / step)\n",
    "            v_ips = 0\n",
    "            step = 1\n",
    "        item = data['interactions'].iloc[i][data_description['items']]\n",
    "        user = data['interactions'].iloc[i][data_description['users']]\n",
    "        reward = data['interactions'].iloc[i][data_description['feedback']]\n",
    "        \n",
    "        user_features = hstack(\n",
    "            [\n",
    "                csr_matrix(([], ([], [])), shape=(1, interactions_matrix.shape[0])),\n",
    "                csr_matrix(data['user_features_test'].values[user, :]),\n",
    "            ]).tocsr()\n",
    "        lfm_scores = lightfm_scoring(lfm, user_features, item_features_test, data_description).squeeze()\n",
    "        lfm_scores[lfm_scores < 0] = 0.0\n",
    "        lfm_scores = lfm_scores / lfm_scores.sum()\n",
    "        \n",
    "        v_ips += reward * lfm_scores[item] / (1 / 100)\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "    return ips_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips = offpolicy_ips(lfm, bandit_data, data_description, batchsize=100)\n",
    "print(f'{np.mean(ips)} +- {np.std(ips)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
